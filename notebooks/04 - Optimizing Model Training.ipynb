{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 4 - Optimizing Model Training\n",
    "\n",
    "In [the previous exercise](./03%20-%20Compute%20Contexts.ipynb), you created cloud-based compute and used it when running a model training experiment. The benefit of cloud compute is that it offers a cost-effective way to scale out your experiment workflow and try different algorithms and parameters in order to optimize your model's performance; and that's what we'll explore in this exercise.\n",
    "\n",
    "> **Important**: This exercise assumes you have completed the previous exercises in this series - specifically, you must have:\n",
    ">\n",
    "> - Created an Azure ML Workspace.\n",
    "> - Uploaded the diabetes.csv data file to the workspace's default datastore.\n",
    "> - Registered a **Diabetes Dataset** dataset in the workspace.\n",
    "> - Provisioned an Azure ML Compute resource named **cpu-cluster**.\n",
    ">\n",
    "> If you haven't done that, now would be a good time - nobody's going to do it for you!\n",
    "\n",
    "## Task 1: Connect to Your Workspace\n",
    "\n",
    "The first thing you need to do is to connect to your workspace using the Azure ML SDK. Let's start by ensuring you still have the latest version installed (if you ended and restarted your Azure Notebooks session, the environment may have been reset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ready to use Azure ML 1.0.69\n"
     ]
    }
   ],
   "source": [
    "#!pip install --upgrade azureml-sdk[notebooks,automl,explain]\n",
    "\n",
    "import azureml.core\n",
    "print(\"Ready to use Azure ML\", azureml.core.VERSION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you're ready to connect to your workspace. When you created it in the previous exercise, you saved its configuration; so now you can simply load the workspace from its configuration file.\n",
    "\n",
    "> **Note**: If the authenticated session with your Azure subscription has expired since you completed the previous exercise, you'll be prompted to reauthenticate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ready to work with ML_Service_Demo\n"
     ]
    }
   ],
   "source": [
    "from azureml.core import Workspace\n",
    "\n",
    "# Load the workspace from the saved config file\n",
    "ws = Workspace.from_config()\n",
    "print('Ready to work with', ws.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's get the Azure ML compute resource you created previously (or recreate it if you deleted it!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing cluster, use it.\n",
      "Succeeded\n",
      "AmlCompute wait for completion finished\n",
      "Minimum number of nodes requested have been provisioned\n"
     ]
    }
   ],
   "source": [
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "\n",
    "# Choose a name for your CPU cluster\n",
    "cpu_cluster_name = \"cpu-cluster\"\n",
    "\n",
    "# Verify that cluster does not exist already\n",
    "try:\n",
    "    cpu_cluster = ComputeTarget(workspace=ws, name=cpu_cluster_name)\n",
    "    print('Found existing cluster, use it.')\n",
    "except ComputeTargetException:\n",
    "    # Create an AzureMl Compute resource (a container cluster)\n",
    "    compute_config = AmlCompute.provisioning_configuration(vm_size='STANDARD_D2_V2', \n",
    "                                                           vm_priority='lowpriority', \n",
    "                                                           max_nodes=4)\n",
    "    cpu_cluster = ComputeTarget.create(ws, cpu_cluster_name, compute_config)\n",
    "\n",
    "cpu_cluster.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Use *Hyperdrive* to Determine Optimal Parameter Values\n",
    "\n",
    "The remote compute you created is a four-node cluster, and you can take advantage of this to execute multiple experiment runs in parallel. One key reason to do this is to try training a model with a range of different hyperparameter values.\n",
    "\n",
    "Azure ML includes a feature called *hyperdrive* that enables you to randomly try different values for one or more hyperparameters, and find the best performing trained model based on a metric that you specify - such as *Accuracy* or *Area Under the Curve (AUC)*.\n",
    "\n",
    "> **More Information**: For more information about Hyperdrive, see the [Azure ML documentation](https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-tune-hyperparameters).\n",
    "\n",
    "Let's run a Hyperdrive experiment on the remote compute you have provisioned. First, we'll create the experiment and its associated folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment: diabetes_training\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from azureml.core import Experiment\n",
    "\n",
    "# Create an experiment\n",
    "experiment_name = 'diabetes_training'\n",
    "experiment = Experiment(workspace = ws, name = experiment_name)\n",
    "\n",
    "# Create a folder for the experiment files\n",
    "experiment_folder = './' + experiment_name\n",
    "os.makedirs(experiment_folder, exist_ok=True)\n",
    "\n",
    "print(\"Experiment:\", experiment.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll create the Python script our experiment will run in order to train a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./diabetes_training/diabetes_training.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile $experiment_folder/diabetes_training.py\n",
    "# Import libraries\n",
    "import argparse\n",
    "import joblib\n",
    "from azureml.core import Workspace, Dataset, Experiment, Run\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "# Set regularization parameter\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--regularization', type=float, dest='reg_rate', default=0.01, help='regularization rate')\n",
    "args = parser.parse_args()\n",
    "reg = args.reg_rate\n",
    "\n",
    "# Get the experiment run context\n",
    "run = Run.get_context()\n",
    "\n",
    "# load the diabetes dataset\n",
    "dataset_name = 'Diabetes Dataset'\n",
    "print(\"Loading data from \" + dataset_name)\n",
    "diabetes = Dataset.get_by_name(workspace=run.experiment.workspace, name=dataset_name).to_pandas_dataframe()\n",
    "\n",
    "# Separate features and labels\n",
    "X, y = diabetes[['Pregnancies','PlasmaGlucose','DiastolicBloodPressure','TricepsThickness','SerumInsulin','BMI','DiabetesPedigree','Age']].values, diabetes['Diabetic'].values\n",
    "\n",
    "# Split data into training set and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=0)\n",
    "\n",
    "# Train a logistic regression model\n",
    "print('Training a logistic regression model with regularization rate of', reg)\n",
    "run.log('Regularization Rate',  np.float(reg))\n",
    "model = LogisticRegression(C=1/reg, solver=\"liblinear\").fit(X_train, y_train)\n",
    "\n",
    "# calculate accuracy\n",
    "y_hat = model.predict(X_test)\n",
    "acc = np.average(y_hat == y_test)\n",
    "print('Accuracy:', acc)\n",
    "run.log('Accuracy', np.float(acc))\n",
    "\n",
    "# calculate AUC\n",
    "y_scores = model.predict_proba(X_test)\n",
    "auc = roc_auc_score(y_test,y_scores[:,1])\n",
    "print('AUC: ' + str(auc))\n",
    "run.log('AUC', np.float(auc))\n",
    "\n",
    "# plot ROC curve\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_scores[:,1])\n",
    "fig = plt.figure(figsize=(6, 4))\n",
    "# Plot the diagonal 50% line\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "# Plot the FPR and TPR achieved by our model\n",
    "plt.plot(fpr, tpr)\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "run.log_image(name = \"ROC\", plot = fig)\n",
    "plt.show()\n",
    "\n",
    "os.makedirs('outputs', exist_ok=True)\n",
    "# note file saved in the outputs folder is automatically uploaded into experiment record\n",
    "joblib.dump(value=model, filename='outputs/diabetes_model.pkl')\n",
    "\n",
    "run.complete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll use the *Hyperdrive* feature of Azure ML to run multiple experiments in parallel, using different values for the **regularization** parameter to find the optimal value for our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84fdccffff2a43f8af9dfd512dfb8934",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "_HyperDriveWidget(widget_settings={'childWidgetDisplay': 'popup', 'send_telemetry': False, 'log_level': 'INFO'…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from azureml.train.hyperdrive import GridParameterSampling, BanditPolicy, HyperDriveConfig, PrimaryMetricGoal\n",
    "from azureml.train.hyperdrive import choice\n",
    "from azureml.widgets import RunDetails\n",
    "from azureml.train.sklearn import SKLearn\n",
    "\n",
    "# Sample a range of parameter values\n",
    "params = GridParameterSampling(\n",
    "    {\n",
    "        # There's only one parameter, so grid sampling will try each value - with multiple parameters it would try every combination\n",
    "        '--regularization': choice(0.001, 0.005, 0.01, 0.05, 0.1, 1.0)\n",
    "    }\n",
    ")\n",
    "\n",
    "# Set evaluation policy to stop poorly performing training runs early\n",
    "policy = BanditPolicy(evaluation_interval=2, slack_factor=0.1)\n",
    "\n",
    "# Create an estimator that uses the remote compute\n",
    "hyper_estimator = SKLearn(source_directory=experiment_folder,\n",
    "                           compute_target = cpu_cluster,\n",
    "                           conda_packages=['pandas','ipykernel','matplotlib'],\n",
    "                           pip_packages=['azureml-sdk','argparse','pyarrow'],\n",
    "                           entry_script='diabetes_training.py')\n",
    "\n",
    "# Configure hyperdrive settings\n",
    "hyperdrive = HyperDriveConfig(estimator=hyper_estimator, \n",
    "                          hyperparameter_sampling=params, \n",
    "                          policy=policy, \n",
    "                          primary_metric_name='AUC', \n",
    "                          primary_metric_goal=PrimaryMetricGoal.MAXIMIZE, \n",
    "                          max_total_runs=6,\n",
    "                          max_concurrent_runs=4)\n",
    "\n",
    "\n",
    "# Run the experiment\n",
    "run = experiment.submit(config=hyperdrive)\n",
    "\n",
    "# Show the status in the notebook as the experiment runs\n",
    "RunDetails(run).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When all of the runs have finished, you can find the best one based on the performance metric you specified (in this case, the one with the best AUC)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Run Id:  diabetes_training_1571322951898254_5\n",
      " -AUC: 0.856969468262725\n",
      " -Accuracy: 0.7891111111111111\n",
      " -Regularization Rate: ['--regularization', '1']\n"
     ]
    }
   ],
   "source": [
    "best_run = run.get_best_run_by_primary_metric()\n",
    "best_run_metrics = best_run.get_metrics()\n",
    "parameter_values = best_run.get_details() ['runDefinition']['arguments']\n",
    "\n",
    "print('Best Run Id: ', best_run.id)\n",
    "print(' -AUC:', best_run_metrics['AUC'])\n",
    "print(' -Accuracy:', best_run_metrics['Accuracy'])\n",
    "print(' -Regularization Rate:',parameter_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we've found the best run, we can register the model it trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "diabetes_model version: 7\n",
      "\t Training context : Hyperdrive\n",
      "\t AUC : 0.856969468262725\n",
      "\t Accuracy : 0.7891111111111111\n",
      "\n",
      "\n",
      "scoring_explainer version: 1\n",
      "\n",
      "\n",
      "automl_model version: 1\n",
      "\n",
      "\n",
      "inception version: 1\n",
      "\t pretrained : inception\n",
      "\n",
      "\n",
      "diabetes_model version: 6\n",
      "\t Training context : Hyperdrive\n",
      "\t AUC : 0.8568562140716564\n",
      "\t Accuracy : 0.7891111111111111\n",
      "\n",
      "\n",
      "diabetes_model version: 5\n",
      "\t Training context : remote compute\n",
      "\t AUC : 0.8568632924585982\n",
      "\t Accuracy : 0.7893333333333333\n",
      "\n",
      "\n",
      "diabetes_model version: 4\n",
      "\t Training context : custom local environment\n",
      "\t AUC : 0.8568371909067503\n",
      "\t Accuracy : 0.7888888888888889\n",
      "\n",
      "\n",
      "diabetes_model version: 3\n",
      "\t Training context : Estimator (from Datasource)\n",
      "\t AUC : 0.8568484720859387\n",
      "\t Accuracy : 0.7891111111111111\n",
      "\n",
      "\n",
      "diabetes_model version: 2\n",
      "\t Training context : Estimator\n",
      "\t AUC : 0.8568515688802257\n",
      "\t Accuracy : 0.7893333333333333\n",
      "\n",
      "\n",
      "diabetes_model version: 1\n",
      "\t Training context : Experiment script\n",
      "\t AUC : 0.85685665647084\n",
      "\t Accuracy : 0.7891111111111111\n",
      "\n",
      "\n",
      "compliance-classifier version: 8\n",
      "\t type : classification\n",
      "\t run_id : 8a129288-febc-4a4e-8cb1-411e78f59ab0\n",
      "\t build_number : 20191002.6\n",
      "\n",
      "\n",
      "compliance-classifier version: 7\n",
      "\t type : classification\n",
      "\t run_id : 5ed15206-af37-4612-b803-2c59423e2bd2\n",
      "\t build_number : 20191002.5\n",
      "\n",
      "\n",
      "compliance-classifier version: 6\n",
      "\t type : classification\n",
      "\t run_id : 6479e0ef-ff08-4d92-8218-9564bda2cd35\n",
      "\t build_number : 20191002.4\n",
      "\n",
      "\n",
      "compliance-classifier version: 5\n",
      "\t type : classification\n",
      "\t run_id : 25b9b2dd-af91-460f-9ecc-65d2dd24a7f9\n",
      "\t build_number : 20191002.3\n",
      "\n",
      "\n",
      "compliance-classifier version: 4\n",
      "\t type : classification\n",
      "\t run_id : 370c80e1-5dfb-4a06-be8e-1c95e5c00e5c\n",
      "\t build_number : 20191002.2\n",
      "\n",
      "\n",
      "compliance-classifier version: 3\n",
      "\t type : classification\n",
      "\t run_id : 2c203100-c77a-4c7d-8b6c-4d17074d0734\n",
      "\t build_number : 20191002.1\n",
      "\n",
      "\n",
      "compliance-classifier version: 2\n",
      "\n",
      "\n",
      "compliance-classifier version: 1\n",
      "\t type : classification\n",
      "\t run_id : deep-learning_1570020996_2e3378b1\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from azureml.core import Model\n",
    "\n",
    "# Register model\n",
    "best_run.register_model(model_path='outputs/diabetes_model.pkl', model_name='diabetes_model', tags={'Training context':'Hyperdrive'}, properties={'AUC': best_run_metrics['AUC'], 'Accuracy': best_run_metrics['Accuracy']})\n",
    "\n",
    "# List registered models\n",
    "for model in Model.list(ws):\n",
    "    print(model.name, 'version:', model.version)\n",
    "    for tag_name in model.tags:\n",
    "        tag = model.tags[tag_name]\n",
    "        print ('\\t',tag_name, ':', tag)\n",
    "    for prop_name in model.properties:\n",
    "        prop = model.properties[prop_name]\n",
    "        print ('\\t',prop_name, ':', prop)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Use *Auto ML* to Find the Best Model\n",
    "\n",
    "Hyperparameter tuning has helped us find the optimal regularization rate for our logistic regression model, but we might get better results by trying a different algorithm, and by performing some basic feature-engineering, such as scaling numeric feature values. You could just create lots of different training scripts that apply various scikit-learn algorithms, and try them all until you find the best result; but Azure ML provides a feature called *Automated Machine Learning* (or *Auto ML*) that can do this for you.\n",
    "\n",
    "First, let's create a folder for a new experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "automl_experiment folder created\n"
     ]
    }
   ],
   "source": [
    "# Create a project folder if it doesn't exist\n",
    "automl_folder = \"automl_experiment\"\n",
    "if not os.path.exists(automl_folder):\n",
    "    os.makedirs(automl_folder)\n",
    "print(automl_folder, 'folder created')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You don't need to create a training script (Auto ML will do that for you), but you do need to load the training data; and when using remote compute, this is best achieved by creating a script containing a **get_data** function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting automl_experiment/get_data.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile $automl_folder/get_data.py\n",
    "#Write the get_data file.\n",
    "from azureml.core import Run, Workspace, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def get_data():\n",
    "\n",
    "    # load the diabetes dataset\n",
    "    run = Run.get_context()\n",
    "    dataset_name = 'Diabetes Dataset'\n",
    "    diabetes = Dataset.get_by_name(workspace=run.experiment.workspace, name=dataset_name).to_pandas_dataframe()\n",
    "\n",
    "    # Separate features and labels\n",
    "    X, y = diabetes[['Pregnancies','PlasmaGlucose','DiastolicBloodPressure','TricepsThickness','SerumInsulin','BMI','DiabetesPedigree','Age']].values, diabetes['Diabetic'].values\n",
    "    \n",
    "    # Split data into training set and test set\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=0)\n",
    "\n",
    "    return { \"X\" : X_train, \"y\" : y_train, \"X_valid\" : X_test, \"y_valid\" : y_test }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you're ready to confifure the Auto ML experiment. To do this, you'll need a run configuration that includes the required packages for the experiment environment, and a set of configuration settings that tells Auto ML how many options to try, which metric to use when evaluating models, and so on.\n",
    "\n",
    "> **More Information**: For more information about options when using Auto ML, see the [Azure ML documentation](https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-configure-auto-train)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.runconfig import RunConfiguration\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "from azureml.train.automl import AutoMLConfig\n",
    "import time\n",
    "import logging\n",
    "\n",
    "\n",
    "automl_run_config = RunConfiguration(framework=\"python\")\n",
    "automl_run_config.environment.docker.enabled = True\n",
    "\n",
    "auto_ml_dependencies = CondaDependencies.create(\n",
    "    pip_packages=[\"azureml-sdk\", \"pyarrow\", \"pandas\", \"scikit-learn\", \"numpy\"])\n",
    "automl_run_config.environment.python.conda_dependencies = auto_ml_dependencies\n",
    "\n",
    "\n",
    "automl_settings = {\n",
    "    \"name\": \"Diabetes_AutoML_{0}\".format(time.time()),\n",
    "    \"iteration_timeout_minutes\": 10,\n",
    "    \"iterations\": 10,\n",
    "    \"primary_metric\": 'AUC_weighted',\n",
    "    \"preprocess\": False,\n",
    "    \"max_concurrent_iterations\": 4,\n",
    "    \"verbosity\": logging.INFO\n",
    "}\n",
    "\n",
    "automl_config = AutoMLConfig(task='classification',\n",
    "                             debug_log='automl_errors.log',\n",
    "                             path=automl_folder,\n",
    "                             compute_target=cpu_cluster,\n",
    "                             run_configuration=automl_run_config,\n",
    "                             data_script=automl_folder + \"/get_data.py\",\n",
    "                             model_explainability=True,\n",
    "                             **automl_settings,\n",
    "                             )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, we're ready to go. Let's start the Auto ML run, which will generate child runs for different algorithms.\n",
    "\n",
    "> **Note**: This will take some time. Progress will be displayed as each child run completes, and then a widget showing the results will be displayed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on remote or ADB.\n",
      "Running on remote compute: cpu-cluster\n",
      "Parent Run ID: AutoML_6104621c-f648-4516-9ae2-d5ad80ac8139\n",
      "Current status: ModelSelection. Beginning model selection.\n",
      "\n",
      "****************************************************************************************************\n",
      "ITERATION: The iteration being evaluated.\n",
      "PIPELINE: A summary description of the pipeline being evaluated.\n",
      "DURATION: Time taken for the current iteration.\n",
      "METRIC: The result of computing score on the fitted pipeline.\n",
      "BEST: The best observed score thus far.\n",
      "****************************************************************************************************\n",
      "\n",
      " ITERATION   PIPELINE                                       DURATION      METRIC      BEST\n"
     ]
    }
   ],
   "source": [
    "from azureml.core.experiment import Experiment\n",
    "from azureml.widgets import RunDetails\n",
    "\n",
    "automl_experiment = Experiment(ws, 'diabetes_automl')\n",
    "automl_run = automl_experiment.submit(automl_config, show_output=True)\n",
    "RunDetails(automl_run).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "View the output of the experiment in the widget, and click the run that produced the best result to see its details.\n",
    "Then click the link to view the experiment details in the Azure portal and view the overall experiment details before viewing the details for the individual run that produced the best result. There's lots of information here about the performance of the model generated and how its features were used.\n",
    "\n",
    "Let's get the best run and the model that was generated (you can ignore any warnings about Azure ML package versions that might appear)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_run, fitted_model = automl_run.get_output()\n",
    "print(best_run)\n",
    "print(fitted_model)\n",
    "best_run_metrics = best_run.get_metrics()\n",
    "for metric_name in best_run_metrics:\n",
    "    metric = best_run_metrics[metric_name]\n",
    "    print(metric_name, metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.train.automl.automl_explain_utilities import AutoMLExplainerSetupClass, automl_setup_model_explanations\n",
    "\n",
    "automl_explainer_setup_obj = automl_setup_model_explanations(fitted_model, X=X_train, \n",
    "                                                             X_test=X_test, y=y_train, \n",
    "                                                             task='classification')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the options you used was to include model *explainability*. This uses a test dataset to evaluate the importance of each feature. You can view this data in the notebook widget or the portal, and you can also retrieve it from the run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING - retrieve_model_explanation() will be deprecated. Please use the workflow described at the link https://github.com/Azure/MachineLearningNotebooks/blob/master/how-to-use-azureml/automated-machine-learning/model-explanation/auto-ml-model-explanation.ipynb to compute explanations for AutoML models and consuming them.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'local_importance_values'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-54a576d98954>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mazureml\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautoml\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautomlexplainer\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mretrieve_model_explanation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mshap_values\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexpected_values\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moverall_summary\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moverall_imp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mper_class_summary\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mper_class_imp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mretrieve_model_explanation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbest_run\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# Overall feature importance (the Feature value is the column index in the training data)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sahieple\\appdata\\local\\continuum\\miniconda3\\envs\\azure_ml\\lib\\site-packages\\azureml\\train\\automl\\automlexplainer.py\u001b[0m in \u001b[0;36mretrieve_model_explanation\u001b[1;34m(child_run)\u001b[0m\n\u001b[0;32m     53\u001b[0m         \u001b[0mexplanation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclient\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdownload_model_explanation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 55\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_convert_explanation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexplanation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     56\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mOptionalDependencyMissingException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sahieple\\appdata\\local\\continuum\\miniconda3\\envs\\azure_ml\\lib\\site-packages\\azureml\\automl\\core\\model_explanation.py\u001b[0m in \u001b[0;36m_convert_explanation\u001b[1;34m(explanation, include_local_importance)\u001b[0m\n\u001b[0;32m     92\u001b[0m     \"\"\"\n\u001b[0;32m     93\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0minclude_local_importance\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 94\u001b[1;33m         \u001b[0mlocal_importance_value\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mexplanation\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlocal_importance_values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     95\u001b[0m     \u001b[0mexpected_value\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mexplanation\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexpected_values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     96\u001b[0m     \u001b[0moverall_summary\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mexplanation\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_ranked_global_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'local_importance_values'"
     ]
    }
   ],
   "source": [
    "from azureml.train.automl.automlexplainer import retrieve_model_explanation\n",
    "\n",
    "shap_values, expected_values, overall_summary, overall_imp, per_class_summary, per_class_imp = retrieve_model_explanation(best_run)\n",
    "\n",
    "# Overall feature importance (the Feature value is the column index in the training data)\n",
    "print(\"Feature\\tImportance\")\n",
    "for i in range(len(overall_imp)):\n",
    "    print(overall_imp[i], '\\t', overall_summary[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'get_feature_importance_dict'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-2f1a6da19cb0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mclient\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mExplanationClient\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_run\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbest_run\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mengineered_explanations\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclient\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdownload_model_explanation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mengineered_explanations\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_feature_importance_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'get_feature_importance_dict'"
     ]
    }
   ],
   "source": [
    "from azureml.explain.model._internal.explanation_client import ExplanationClient\n",
    "\n",
    "client = ExplanationClient.from_run(best_run)\n",
    "engineered_explanations = client.download_model_explanation(raw=False)\n",
    "print(engineered_explanations.get_feature_importance_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'get_feature_importance_dict'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-fb9994f8a21a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mclient\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mExplanationClient\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_run\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbest_run\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mraw_explanations\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclient\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdownload_model_explanation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw_explanations\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_feature_importance_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'get_feature_importance_dict'"
     ]
    }
   ],
   "source": [
    "\n",
    "client = ExplanationClient.from_run(best_run)\n",
    "raw_explanations = client.download_model_explanation(raw=True)\n",
    "print(raw_explanations.get_feature_importance_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, having found the best performing model, you can register it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register model\n",
    "best_run.register_model(model_path='outputs/model.pkl', model_name='diabetes_model', tags={'Training context':'Auto ML'}, properties={'AUC': best_run_metrics['AUC_weighted'], 'Accuracy': best_run_metrics['accuracy']})\n",
    "\n",
    "# List registered models\n",
    "for model in Model.list(ws):\n",
    "    print(model.name, 'version:', model.version)\n",
    "    for tag_name in model.tags:\n",
    "        tag = model.tags[tag_name]\n",
    "        print ('\\t',tag_name, ':', tag)\n",
    "    for prop_name in model.properties:\n",
    "        prop = model.properties[prop_name]\n",
    "        print ('\\t',prop_name, ':', prop)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you've seen several ways to leverage the high-scale compute capabilities of the cloud to experiment with model training and find the best performing model for your data. In the next exerise, you'll deploy a registered model into production."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
