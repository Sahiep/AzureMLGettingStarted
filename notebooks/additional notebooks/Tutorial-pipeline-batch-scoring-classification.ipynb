{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved.  \n",
    "Licensed under the MIT License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Impressions](https://PixelServer20190423114238.azurewebsites.net/api/impressions/MachineLearningNotebooks/how-to-use-azureml/machine-learning-pipelines/pipeline-batch-scoring/pipeline-batch-scoring.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use Azure Machine Learning Pipelines for batch prediction\n",
    "\n",
    "In this tutorial, you use Azure Machine Learning service pipelines to run a batch scoring image classification job. The example job uses the pre-trained [Inception-V3](https://arxiv.org/abs/1512.00567) CNN (convolutional neural network) Tensorflow model to classify unlabeled images. Machine learning pipelines optimize your workflow with speed, portability, and reuse so you can focus on your expertise, machine learning, rather than on infrastructure and automation. After building and publishing a pipeline, you can configure a REST endpoint to enable triggering the pipeline from any HTTP library on any platform.\n",
    "\n",
    "\n",
    "In this tutorial, you learn the following tasks:\n",
    "\n",
    "> * Configure workspace and download sample data\n",
    "> * Create data objects to fetch and output data\n",
    "> * Download, prepare, and register the model to your workspace\n",
    "> * Provision compute targets and create a scoring script\n",
    "> * Build, run, and publish a pipeline\n",
    "> * Enable a REST endpoint for the pipeline\n",
    "\n",
    "If you donâ€™t have an Azure subscription, create a free account before you begin. Try the [free or paid version of Azure Machine Learning service](https://aka.ms/AMLFree) today."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "* Complete the [setup tutorial](https://docs.microsoft.com/azure/machine-learning/service/tutorial-1st-experiment-sdk-setup) if you don't already have an Azure Machine Learning service workspace or notebook virtual machine.\n",
    "* After you complete the setup tutorial, open the **tutorials/tutorial-pipeline-batch-scoring-classification.ipynb** notebook using the same notebook server.\n",
    "\n",
    "This tutorial is also available on [GitHub](https://github.com/Azure/MachineLearningNotebooks/tree/master/tutorials) if you wish to run it in your own [local environment](how-to-configure-environment.md#local). Run `pip install azureml-sdk[notebooks] azureml-pipeline-core azureml-pipeline-steps pandas requests` to get the required packages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure workspace and create datastore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a workspace object from the existing workspace. A [Workspace](https://docs.microsoft.com/python/api/azureml-core/azureml.core.workspace.workspace?view=azure-ml-py) is a class that accepts your Azure subscription and resource information. It also creates a cloud resource to monitor and track your model runs. `Workspace.from_config()` reads the file **config.json** and loads the authentication details into an object named `ws`. `ws` is used throughout the rest of the code in this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ML_Service_Demo found.\n",
      "ML_Service_Demo saved.\n"
     ]
    }
   ],
   "source": [
    "from azureml.core import Workspace\n",
    "\n",
    "SUBSCRIPTION_ID = '7d48758f-d40b-4252-854c-e7d8f2ed7645' # Get this from the Azure portal\n",
    "RESOURCE_GROUP_NAME  = 'AzureMLDemo' # Or any resource group name of your choice - if it doesn't exist, it will be created\n",
    "WORKSPACE_NAME  = 'ML_Service_Demo' # Or a name of your choice - if it doesn't exist, it will be created\n",
    "REGION = 'westeurope'# Or a region of your choice\n",
    "\n",
    "ws = None\n",
    "try:\n",
    "    # Find existing workspace\n",
    "    ws = Workspace(workspace_name=WORKSPACE_NAME,\n",
    "                   subscription_id=SUBSCRIPTION_ID,\n",
    "                   resource_group= RESOURCE_GROUP_NAME)\n",
    "    print (ws.name, \"found.\")\n",
    "except Exception as ex:\n",
    "    # If workspace not found, create it\n",
    "    print(ex.message)\n",
    "    print(\"Attempting to create new workspace...\")\n",
    "    ws = Workspace.create(name=WORKSPACE_NAME, \n",
    "                      subscription_id=SUBSCRIPTION_ID,\n",
    "                      resource_group=RESOURCE_GROUP_NAME,\n",
    "                      create_resource_group=True,\n",
    "                      location=REGION \n",
    "                     )\n",
    "    print(ws.name, \"created.\")\n",
    "finally:\n",
    "    # Save the workspace configuration for later\n",
    "    if ws != None:\n",
    "        ws.write_config()\n",
    "        print(ws.name, \"saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Workspace\n",
    "ws = Workspace.from_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a datastore for sample images\n",
    "\n",
    "Get the ImageNet evaluation public data sample from the public blob container `sampledata` on the account `pipelinedata`. Calling `register_azure_blob_container()` makes the data available to the workspace under the name `images_datastore`. Then specify the workspace default datastore as the output datastore, which you use for scoring output in the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.datastore import Datastore\n",
    "\n",
    "batchscore_blob = Datastore.register_azure_blob_container(ws, \n",
    "                      datastore_name=\"images_datastore\", \n",
    "                      container_name=\"sampledata\", \n",
    "                      account_name=\"pipelinedata\", \n",
    "                      overwrite=True)\n",
    "\n",
    "def_data_store = ws.get_default_datastore()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create data objects\n",
    "\n",
    "When building pipelines, `DataReference` objects are used for reading data from workspace datastores, and `PipelineData` objects are used for transferring intermediate data between pipeline steps.\n",
    "\n",
    "This batch scoring example only uses one pipeline step, but in use-cases with multiple steps, the typical flow will include:\n",
    "\n",
    "1. Using `DataReference` objects as **inputs** to fetch raw data, performing some transformations, then **outputting** a `PipelineData` object.\n",
    "1. Use the previous step's `PipelineData` **output object** as an *input object*, repeated for subsequent steps.\n",
    "\n",
    "For this scenario you create `DataReference` objects corresponding to the datastore directories for both the input images and the classification labels (y-test values). You also create a `PipelineData` object for the batch scoring output data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.data.data_reference import DataReference\n",
    "from azureml.pipeline.core import PipelineData\n",
    "\n",
    "input_images = DataReference(datastore=batchscore_blob, \n",
    "                             data_reference_name=\"input_images\",\n",
    "                             path_on_datastore=\"batchscoring/images\",\n",
    "                             mode=\"download\"\n",
    "                            )\n",
    "\n",
    "label_dir = DataReference(datastore=batchscore_blob, \n",
    "                          data_reference_name=\"input_labels\",\n",
    "                          path_on_datastore=\"batchscoring/labels\",\n",
    "                          mode=\"download\"                          \n",
    "                         )\n",
    "\n",
    "output_dir = PipelineData(name=\"scores\", \n",
    "                          datastore=def_data_store, \n",
    "                          output_path_on_compute=\"batchscoring/results\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download and register the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the pre-trained Tensorflow model to use it for batch scoring in the pipeline. First create a local directory where you store the model, then download and extract it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tarfile\n",
    "import urllib.request\n",
    "\n",
    "if not os.path.isdir(\"models\"):\n",
    "    os.mkdir(\"models\")\n",
    "    \n",
    "response = urllib.request.urlretrieve(\"http://download.tensorflow.org/models/inception_v3_2016_08_28.tar.gz\", \"model.tar.gz\")\n",
    "tar = tarfile.open(\"model.tar.gz\", \"r:gz\")\n",
    "tar.extractall(\"models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you register the model to your workspace, which allows you to easily retrieve it in the pipeline process. In the `register()` static function, the `model_name` parameter is the key you use to locate your model throughout the SDK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Registering model inception\n"
     ]
    }
   ],
   "source": [
    "from azureml.core.model import Model\n",
    " \n",
    "model = Model.register(model_path=\"models/inception_v3.ckpt\",\n",
    "                       model_name=\"inception\",\n",
    "                       tags={\"pretrained\": \"inception\"},\n",
    "                       description=\"Imagenet trained tensorflow inception\",\n",
    "                       workspace=ws)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create and attach remote compute target\n",
    "\n",
    "Azure Machine Learning service pipelines cannot be run locally, and only run on cloud resources. Remote compute targets are reusable virtual compute environments where you run experiments and work-flows. Run the following code to create a GPU-enabled [`AmlCompute`](https://docs.microsoft.com/python/api/azureml-core/azureml.core.compute.amlcompute.amlcompute?view=azure-ml-py) target, and attach it to your workspace. See the [conceptual article](https://docs.microsoft.com/azure/machine-learning/service/concept-compute-target) for more information on compute targets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.compute import AmlCompute, ComputeTarget\n",
    "from azureml.exceptions import ComputeTargetException\n",
    "compute_name = \"gpu-cluster\"\n",
    "\n",
    "# checks to see if compute target already exists in workspace, else create it\n",
    "try:\n",
    "    compute_target = ComputeTarget(workspace=ws, name=compute_name)\n",
    "except ComputeTargetException:\n",
    "    config = AmlCompute.provisioning_configuration(vm_size=\"STANDARD_NC6\",\n",
    "                                                   vm_priority=\"lowpriority\", \n",
    "                                                   min_nodes=0, \n",
    "                                                   max_nodes=1)\n",
    "\n",
    "    compute_target = ComputeTarget.create(workspace=ws, name=compute_name, provisioning_configuration=config)\n",
    "    compute_target.wait_for_completion(show_output=True, min_node_count=None, timeout_in_minutes=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write a scoring script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do the scoring, you create a batch scoring script `batch_scoring.py`, and write it to the current directory. The script takes input images, applies the classification model, and outputs the predictions to a results file.\n",
    "\n",
    "The script `batch_scoring.py` takes the following parameters, which get passed from the `PythonScriptStep` that you create later:\n",
    "\n",
    "- `--model_name`: the name of the model being used\n",
    "- `--label_dir` : the directory holding the `labels.txt` file \n",
    "- `--dataset_path`: the directory containing the input images\n",
    "- `--output_dir` : the script will run the model on the data and output a `results-label.txt` to this directory\n",
    "- `--batch_size` : the batch size used in running the model\n",
    "\n",
    "The pipelines infrastructure uses the `ArgumentParser` class to pass parameters into pipeline steps. For example, in the code below the first argument `--model_name` is given the property identifier `model_name`. In the `main()` function, this property is accessed using `Model.get_model_path(args.model_name)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing batch_scoring.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile batch_scoring.py\n",
    "\n",
    "import os\n",
    "import argparse\n",
    "import datetime\n",
    "import time\n",
    "import tensorflow as tf\n",
    "from math import ceil\n",
    "import numpy as np\n",
    "import shutil\n",
    "from tensorflow.contrib.slim.python.slim.nets import inception_v3\n",
    "from azureml.core.model import Model\n",
    "\n",
    "slim = tf.contrib.slim\n",
    "\n",
    "parser = argparse.ArgumentParser(description=\"Start a tensorflow model serving\")\n",
    "parser.add_argument('--model_name', dest=\"model_name\", required=True)\n",
    "parser.add_argument('--label_dir', dest=\"label_dir\", required=True)\n",
    "parser.add_argument('--dataset_path', dest=\"dataset_path\", required=True)\n",
    "parser.add_argument('--output_dir', dest=\"output_dir\", required=True)\n",
    "parser.add_argument('--batch_size', dest=\"batch_size\", type=int, required=True)\n",
    "\n",
    "args = parser.parse_args()\n",
    "\n",
    "image_size = 299\n",
    "num_channel = 3\n",
    "\n",
    "# create output directory if it does not exist\n",
    "os.makedirs(args.output_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "def get_class_label_dict(label_file):\n",
    "    label = []\n",
    "    proto_as_ascii_lines = tf.gfile.GFile(label_file).readlines()\n",
    "    for l in proto_as_ascii_lines:\n",
    "        label.append(l.rstrip())\n",
    "    return label\n",
    "\n",
    "\n",
    "class DataIterator:\n",
    "    def __init__(self, data_dir):\n",
    "        self.file_paths = []\n",
    "        image_list = os.listdir(data_dir)\n",
    "        self.file_paths = [data_dir + '/' + file_name.rstrip() for file_name in image_list]\n",
    "\n",
    "        self.labels = [1 for file_name in self.file_paths]\n",
    "\n",
    "    @property\n",
    "    def size(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def input_pipeline(self, batch_size):\n",
    "        images_tensor = tf.convert_to_tensor(self.file_paths, dtype=tf.string)\n",
    "        labels_tensor = tf.convert_to_tensor(self.labels, dtype=tf.int64)\n",
    "        input_queue = tf.train.slice_input_producer([images_tensor, labels_tensor], shuffle=False)\n",
    "        labels = input_queue[1]\n",
    "        images_content = tf.read_file(input_queue[0])\n",
    "\n",
    "        image_reader = tf.image.decode_jpeg(images_content, channels=num_channel, name=\"jpeg_reader\")\n",
    "        float_caster = tf.cast(image_reader, tf.float32)\n",
    "        new_size = tf.constant([image_size, image_size], dtype=tf.int32)\n",
    "        images = tf.image.resize_images(float_caster, new_size)\n",
    "        images = tf.divide(tf.subtract(images, [0]), [255])\n",
    "\n",
    "        image_batch, label_batch = tf.train.batch([images, labels], batch_size=batch_size, capacity=5 * batch_size)\n",
    "        return image_batch\n",
    "\n",
    "\n",
    "def main(_):\n",
    "    label_file_name = os.path.join(args.label_dir, \"labels.txt\")\n",
    "    label_dict = get_class_label_dict(label_file_name)\n",
    "    classes_num = len(label_dict)\n",
    "    test_feeder = DataIterator(data_dir=args.dataset_path)\n",
    "    total_size = len(test_feeder.labels)\n",
    "    count = 0\n",
    "    \n",
    "    # get model from model registry\n",
    "    model_path = Model.get_model_path(args.model_name)\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        test_images = test_feeder.input_pipeline(batch_size=args.batch_size)\n",
    "        with slim.arg_scope(inception_v3.inception_v3_arg_scope()):\n",
    "            input_images = tf.placeholder(tf.float32, [args.batch_size, image_size, image_size, num_channel])\n",
    "            logits, _ = inception_v3.inception_v3(input_images,\n",
    "                                                  num_classes=classes_num,\n",
    "                                                  is_training=False)\n",
    "            probabilities = tf.argmax(logits, 1)\n",
    "\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        sess.run(tf.local_variables_initializer())\n",
    "        coord = tf.train.Coordinator()\n",
    "        threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n",
    "        saver = tf.train.Saver()\n",
    "        saver.restore(sess, model_path)\n",
    "        out_filename = os.path.join(args.output_dir, \"result-labels.txt\")\n",
    "        with open(out_filename, \"w\") as result_file:\n",
    "            i = 0\n",
    "            while count < total_size and not coord.should_stop():\n",
    "                test_images_batch = sess.run(test_images)\n",
    "                file_names_batch = test_feeder.file_paths[i * args.batch_size:\n",
    "                                                          min(test_feeder.size, (i + 1) * args.batch_size)]\n",
    "                results = sess.run(probabilities, feed_dict={input_images: test_images_batch})\n",
    "                new_add = min(args.batch_size, total_size - count)\n",
    "                count += new_add\n",
    "                i += 1\n",
    "                for j in range(new_add):\n",
    "                    result_file.write(os.path.basename(file_names_batch[j]) + \": \" + label_dict[results[j]] + \"\\n\")\n",
    "                result_file.flush()\n",
    "            coord.request_stop()\n",
    "            coord.join(threads)\n",
    "\n",
    "        shutil.copy(out_filename, \"./outputs/\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    tf.app.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pipeline in this tutorial only has one step and writes the output to a file, but for multi-step pipelines, you also use `ArgumentParser` to define a directory to write output data for input to subsequent steps. See the [notebook](https://github.com/Azure/MachineLearningNotebooks/blob/master/how-to-use-azureml/machine-learning-pipelines/nyc-taxi-data-regression-model-building/nyc-taxi-data-regression-model-building.ipynb) for an example of passing data between multiple pipeline steps using the `ArgumentParser` design pattern."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build and run the pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before running the pipeline, you create an object that defines the python environment and dependencies needed by your script `batch_scoring.py`. The main dependency required is Tensorflow, but you also install `azureml-defaults` for background processes from the SDK. Create a `RunConfiguration` object using the dependencies, and also specify Docker and Docker-GPU support."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.runconfig import DEFAULT_GPU_IMAGE\n",
    "from azureml.core.runconfig import CondaDependencies, RunConfiguration\n",
    "\n",
    "cd = CondaDependencies.create(pip_packages=[\"tensorflow-gpu==1.13.1\", \"azureml-defaults\"])\n",
    "\n",
    "amlcompute_run_config = RunConfiguration(conda_dependencies=cd)\n",
    "amlcompute_run_config.environment.docker.enabled = True\n",
    "amlcompute_run_config.environment.docker.base_image = DEFAULT_GPU_IMAGE\n",
    "amlcompute_run_config.environment.spark.precache_packages = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameterize the pipeline\n",
    "\n",
    "Define a custom parameter for the pipeline to control the batch size. After the pipeline has been published and exposed via a REST endpoint, any configured parameters are also exposed and can be specified in the JSON payload when rerunning the pipeline with an HTTP request.\n",
    "\n",
    "Create a `PipelineParameter` object to enable this behavior, and define a name and default value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.pipeline.core.graph import PipelineParameter\n",
    "batch_size_param = PipelineParameter(name=\"param_batch_size\", default_value=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the pipeline step\n",
    "\n",
    "A pipeline step is an object that encapsulates everything you need for running a pipeline including:\n",
    "\n",
    "* environment and dependency settings\n",
    "* the compute resource to run the pipeline on\n",
    "* input and output data, and any custom parameters\n",
    "* reference to a script or SDK-logic to run during the step\n",
    "\n",
    "There are multiple classes that inherit from the parent class [`PipelineStep`](https://docs.microsoft.com/python/api/azureml-pipeline-core/azureml.pipeline.core.builder.pipelinestep?view=azure-ml-py) to assist with building a step using certain frameworks and stacks. In this example, you use the [`PythonScriptStep`](https://docs.microsoft.com/python/api/azureml-pipeline-steps/azureml.pipeline.steps.python_script_step.pythonscriptstep?view=azure-ml-py) class to define your step logic using a custom python script. Note that if an argument to your script is either an input to the step or output of the step, it must be defined **both** in the `arguments` array, **as well as** in either the `input` or `output` parameter, respectively. \n",
    "\n",
    "An object reference in the `outputs` array becomes available as an **input** for a subsequent pipeline step, for scenarios where there is more than one step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.pipeline.steps import PythonScriptStep\n",
    "\n",
    "batch_score_step = PythonScriptStep(\n",
    "    name=\"batch_scoring\",\n",
    "    script_name=\"batch_scoring.py\",\n",
    "    arguments=[\"--dataset_path\", input_images, \n",
    "               \"--model_name\", \"inception\",\n",
    "               \"--label_dir\", label_dir, \n",
    "               \"--output_dir\", output_dir, \n",
    "               \"--batch_size\", batch_size_param],\n",
    "    compute_target=compute_target,\n",
    "    inputs=[input_images, label_dir],\n",
    "    outputs=[output_dir],\n",
    "    runconfig=amlcompute_run_config\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a list of all classes for different step types, see the [steps package](https://docs.microsoft.com/python/api/azureml-pipeline-steps/azureml.pipeline.steps?view=azure-ml-py)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the pipeline\n",
    "\n",
    "Now you run the pipeline. First create a `Pipeline` object with your workspace reference and the pipeline step you created. The `steps` parameter is an array of steps, and in this case there is only one step for batch scoring. To build pipelines with multiple steps, you place the steps in order in this array.\n",
    "\n",
    "Next use the `Experiment.submit()` function to submit the pipeline for execution. You also specify the custom parameter `param_batch_size`. The `wait_for_completion` function will output logs during the pipeline build process, which allows you to see current progress.\n",
    "\n",
    "Note: The first pipeline run takes roughly **15 minutes**, as all dependencies must be downloaded, a Docker image is created, and the Python environment is provisioned/created. Running it again takes significantly less time as those resources are reused. However, total run time depends on the workload of your scripts and processes running in each pipeline step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING - 'gpu_support' is no longer necessary; AzureML now automatically detects and uses nvidia docker extension when it is available. It will be removed in a future release.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created step batch_scoring [79192b50][b80af712-a7ce-4a13-b9bd-2bf73ba38555], (This step will run and generate new outputs)\n",
      "Created data reference input_images for StepId [fc5385b6][10408fb6-a643-4762-a4d9-c871acc9c8a2], (Consumers of this data will generate new runs.)\n",
      "Created data reference input_labels for StepId [cae50243][19b82db7-6f98-4a8b-864f-60324b8fe3be], (Consumers of this data will generate new runs.)\n",
      "Submitted PipelineRun 07abfe6a-33ed-4f20-83ef-ba82ce11727a\n",
      "Link to Azure Portal: https://mlworkspace.azure.ai/portal/subscriptions/7d48758f-d40b-4252-854c-e7d8f2ed7645/resourceGroups/AzureMLDemo/providers/Microsoft.MachineLearningServices/workspaces/ML_Service_Demo/experiments/batch_scoring/runs/07abfe6a-33ed-4f20-83ef-ba82ce11727a\n",
      "PipelineRunId: 07abfe6a-33ed-4f20-83ef-ba82ce11727a\n",
      "Link to Portal: https://mlworkspace.azure.ai/portal/subscriptions/7d48758f-d40b-4252-854c-e7d8f2ed7645/resourceGroups/AzureMLDemo/providers/Microsoft.MachineLearningServices/workspaces/ML_Service_Demo/experiments/batch_scoring/runs/07abfe6a-33ed-4f20-83ef-ba82ce11727a\n",
      "PipelineRun Status: Running\n",
      "\n",
      "\n",
      "StepRunId: bbc67064-59c5-4fa1-8baa-fec778ff3f33\n",
      "Link to Portal: https://mlworkspace.azure.ai/portal/subscriptions/7d48758f-d40b-4252-854c-e7d8f2ed7645/resourceGroups/AzureMLDemo/providers/Microsoft.MachineLearningServices/workspaces/ML_Service_Demo/experiments/batch_scoring/runs/bbc67064-59c5-4fa1-8baa-fec778ff3f33\n",
      "StepRun( batch_scoring ) Status: NotStarted\n",
      "StepRun( batch_scoring ) Status: Running\n",
      "\n",
      "Streaming azureml-logs/20_image_build_log.txt\n",
      "=============================================\n",
      "2019/10/17 11:11:34 Downloading source code...\n",
      "2019/10/17 11:11:36 Finished downloading source code\n",
      "2019/10/17 11:11:36 Creating Docker network: acb_default_network, driver: 'bridge'\n",
      "2019/10/17 11:11:36 Successfully set up Docker network: acb_default_network\n",
      "2019/10/17 11:11:36 Setting up Docker configuration...\n",
      "2019/10/17 11:11:37 Successfully set up Docker configuration\n",
      "2019/10/17 11:11:37 Logging in to registry: mlservicedem1fffd9c8.azurecr.io\n",
      "2019/10/17 11:11:38 Successfully logged into mlservicedem1fffd9c8.azurecr.io\n",
      "2019/10/17 11:11:38 Executing step ID: acb_step_0. Timeout(sec): 5400, Working directory: '', Network: 'acb_default_network'\n",
      "2019/10/17 11:11:38 Scanning for dependencies...\n",
      "2019/10/17 11:11:39 Successfully scanned dependencies\n",
      "2019/10/17 11:11:39 Launching container with name: acb_step_0\n",
      "Sending build context to Docker daemon  59.39kB\n",
      "\n",
      "Step 1/14 : FROM mcr.microsoft.com/azureml/base-gpu:intelmpi2018.3-cuda10.0-cudnn7-ubuntu16.04@sha256:b747b5a1dbdb93aea632b14764219c834dae19e09433fb29729b67d05548756e\n",
      "sha256:b747b5a1dbdb93aea632b14764219c834dae19e09433fb29729b67d05548756e: Pulling from azureml/base-gpu\n",
      "f7277927d38a: Already exists\n",
      "8d3eac894db4: Already exists\n",
      "edf72af6d627: Already exists\n",
      "3e4f86211d23: Already exists\n",
      "d6e9603ff777: Pulling fs layer\n",
      "5cad422780e2: Pulling fs layer\n",
      "8130687c8acb: Pulling fs layer\n",
      "c11e9246d621: Pulling fs layer\n",
      "0dfae24cbbd9: Pulling fs layer\n",
      "0bb049a6d391: Pulling fs layer\n",
      "988ac06e9a72: Pulling fs layer\n",
      "1d903ffe89fe: Pulling fs layer\n",
      "4c5e9e6c7e14: Pulling fs layer\n",
      "7ebd3e323491: Pulling fs layer\n",
      "8796a8f9e018: Pulling fs layer\n",
      "362d975917fe: Pulling fs layer\n",
      "a0018081f252: Pulling fs layer\n",
      "4c5e9e6c7e14: Waiting\n",
      "7ebd3e323491: Waiting\n",
      "8796a8f9e018: Waiting\n",
      "362d975917fe: Waiting\n",
      "a0018081f252: Waiting\n",
      "c11e9246d621: Waiting\n",
      "0dfae24cbbd9: Waiting\n",
      "0bb049a6d391: Waiting\n",
      "988ac06e9a72: Waiting\n",
      "1d903ffe89fe: Waiting\n",
      "8130687c8acb: Verifying Checksum\n",
      "8130687c8acb: Download complete\n",
      "5cad422780e2: Download complete\n",
      "d6e9603ff777: Verifying Checksum\n",
      "d6e9603ff777: Download complete\n",
      "d6e9603ff777: Pull complete\n",
      "5cad422780e2: Pull complete\n",
      "8130687c8acb: Pull complete\n",
      "c11e9246d621: Verifying Checksum\n",
      "c11e9246d621: Download complete\n",
      "0bb049a6d391: Verifying Checksum\n",
      "0bb049a6d391: Download complete\n",
      "988ac06e9a72: Verifying Checksum\n",
      "988ac06e9a72: Download complete\n",
      "0dfae24cbbd9: Verifying Checksum\n",
      "0dfae24cbbd9: Download complete\n",
      "1d903ffe89fe: Download complete\n",
      "4c5e9e6c7e14: Verifying Checksum\n",
      "4c5e9e6c7e14: Download complete\n",
      "7ebd3e323491: Verifying Checksum\n",
      "7ebd3e323491: Download complete\n",
      "362d975917fe: Verifying Checksum\n",
      "362d975917fe: Download complete\n",
      "8796a8f9e018: Verifying Checksum\n",
      "8796a8f9e018: Download complete\n",
      "a0018081f252: Verifying Checksum\n",
      "a0018081f252: Download complete\n",
      "c11e9246d621: Pull complete\n",
      "0dfae24cbbd9: Pull complete\n",
      "0bb049a6d391: Pull complete\n",
      "988ac06e9a72: Pull complete\n",
      "1d903ffe89fe: Pull complete\n",
      "4c5e9e6c7e14: Pull complete\n",
      "7ebd3e323491: Pull complete\n",
      "8796a8f9e018: Pull complete\n",
      "362d975917fe: Pull complete\n",
      "a0018081f252: Pull complete\n",
      "Digest: sha256:b747b5a1dbdb93aea632b14764219c834dae19e09433fb29729b67d05548756e\n",
      "Status: Downloaded newer image for mcr.microsoft.com/azureml/base-gpu:intelmpi2018.3-cuda10.0-cudnn7-ubuntu16.04@sha256:b747b5a1dbdb93aea632b14764219c834dae19e09433fb29729b67d05548756e\n",
      " ---> b155a67c3bc6\n",
      "Step 2/14 : USER root\n",
      " ---> Running in 5cf37fd242e8\n",
      "Removing intermediate container 5cf37fd242e8\n",
      " ---> 93425007366e\n",
      "Step 3/14 : RUN mkdir -p $HOME/.cache\n",
      " ---> Running in 0e1c17a0c6c9\n",
      "Removing intermediate container 0e1c17a0c6c9\n",
      " ---> 55978f7283f4\n",
      "Step 4/14 : WORKDIR /\n",
      " ---> Running in 4b81ecfe45ac\n",
      "Removing intermediate container 4b81ecfe45ac\n",
      " ---> ee1cd245b39f\n",
      "Step 5/14 : COPY azureml-environment-setup/99brokenproxy /etc/apt/apt.conf.d/\n",
      " ---> 6456ba86364a\n",
      "Step 6/14 : RUN if dpkg --compare-versions `conda --version | grep -oE '[^ ]+$'` lt 4.4.11; then conda install conda==4.4.11; fi\n",
      " ---> Running in 8213cc027df9\n",
      "Removing intermediate container 8213cc027df9\n",
      " ---> 2aecc95ced18\n",
      "Step 7/14 : COPY azureml-environment-setup/mutated_conda_dependencies.yml azureml-environment-setup/mutated_conda_dependencies.yml\n",
      " ---> f2b24f14bd32\n",
      "Step 8/14 : RUN ldconfig /usr/local/cuda/lib64/stubs && conda env create -p /azureml-envs/azureml_29ce53e4b6e031ec3cfd3ffd40786b73 -f azureml-environment-setup/mutated_conda_dependencies.yml && rm -rf \"$HOME/.cache/pip\" && conda clean -aqy && CONDA_ROOT_DIR=$(conda info --root) && rm -rf \"$CONDA_ROOT_DIR/pkgs\" && find \"$CONDA_ROOT_DIR\" -type d -name __pycache__ -exec rm -rf {} + && ldconfig\n",
      " ---> Running in ddc22ca2f45c\n",
      "Solving environment: ...working... done\n",
      "\u001b[91m\n",
      "\n",
      "==> WARNING: A newer version of conda exists. <==\n",
      "  current version: 4.5.11\n",
      "  latest version: 4.7.12\n",
      "\n",
      "Please update conda by running\n",
      "\n",
      "    $ conda update -n base -c defaults conda\n",
      "\n",
      "\n",
      "\n",
      "python-3.6.2         | 19.0 MB   |            |   0% \u001b[0m\u001b[91m\n",
      "python-3.6.2         | 19.0 MB   | ##3        |  24% \u001b[0m\u001b[91m\n",
      "python-3.6.2         | 19.0 MB   | ######4    |  65% \u001b[0m\u001b[91m\n",
      "python-3.6.2         | 19.0 MB   | ########1  |  82% \u001b[0m\u001b[91m\n",
      "python-3.6.2         | 19.0 MB   | #########4 |  94% \u001b[0m\u001b[91m\n",
      "python-3.6.2         | 19.0 MB   | ########## | 100% \u001b[0m\u001b[91m\n",
      "\n",
      "readline-6.2         | 713 KB    |            |   0% \u001b[0m\u001b[91m\n",
      "readline-6.2         | 713 KB    | ########4  |  85% \u001b[0m\u001b[91m\n",
      "readline-6.2         | 713 KB    | ########## | 100% \u001b[0m\u001b[91m\n",
      "\n",
      "sqlite-3.13.0        | 4.9 MB    |            |   0% \u001b[0m\u001b[91m\n",
      "sqlite-3.13.0        | 4.9 MB    | #######5   |  76% \u001b[0m\u001b[91m\n",
      "sqlite-3.13.0        | 4.9 MB    | #########8 |  98% \u001b[0m\u001b[91m\n",
      "sqlite-3.13.0        | 4.9 MB    | ########## | 100% \u001b[0m\u001b[91m\n",
      "\n",
      "openssl-1.0.2r       | 3.1 MB    |            |   0% \u001b[0m\u001b[91m\n",
      "openssl-1.0.2r       | 3.1 MB    | #######7   |  78% \u001b[0m\u001b[91m\n",
      "openssl-1.0.2r       | 3.1 MB    | #########7 |  97% \u001b[0m\u001b[91m\n",
      "openssl-1.0.2r       | 3.1 MB    | ########## | 100% \u001b[0m\u001b[91m\n",
      "\n",
      "certifi-2019.9.11    | 147 KB    |            |   0% \u001b[0m\u001b[91m\n",
      "certifi-2019.9.11    | 147 KB    | ########## | 100% \u001b[0m\u001b[91m\n",
      "\n",
      "pip-19.3             | 1.9 MB    |            |   0% \u001b[0m\u001b[91m\n",
      "pip-19.3             | 1.9 MB    | #######8   |  79% \u001b[0m\u001b[91m\n",
      "pip-19.3             | 1.9 MB    | #########5 |  95% \u001b[0m\u001b[91m\n",
      "pip-19.3             | 1.9 MB    | ########## | 100% \u001b[0m\u001b[91m\n",
      "\n",
      "ca-certificates-2019 | 144 KB    |            |   0% \u001b[0m\u001b[91m\n",
      "ca-certificates-2019 | 144 KB    | ########## | 100% \u001b[0m\u001b[91m\n",
      "\n",
      "tk-8.5.19            | 1.9 MB    |            |   0% \u001b[0m\u001b[91m\n",
      "tk-8.5.19            | 1.9 MB    | #######7   |  78% \u001b[0m\u001b[91m\n",
      "tk-8.5.19            | 1.9 MB    | #########2 |  93% \u001b[0m\u001b[91m\n",
      "tk-8.5.19            | 1.9 MB    | ########## | 100% \u001b[0m\u001b[91m\n",
      "\n",
      "setuptools-41.4.0    | 624 KB    |            |   0% \u001b[0m\u001b[91m\n",
      "setuptools-41.4.0    | 624 KB    | ########4  |  85% \u001b[0m\u001b[91m\n",
      "setuptools-41.4.0    | 624 KB    | ########## | 100% \u001b[0m\u001b[91m\n",
      "\n",
      "wheel-0.33.6         | 35 KB     |            |   0% \u001b[0m\u001b[91m\n",
      "wheel-0.33.6         | 35 KB     | ########## | 100% \u001b[0m\u001b[91m\n",
      "\n",
      "_libgcc_mutex-0.1    | 3 KB      |            |   0% \u001b[0m\u001b[91m\n",
      "_libgcc_mutex-0.1    | 3 KB      | ########## | 100% \u001b[0m\u001b[91m\n",
      "\n",
      "zlib-1.2.11          | 105 KB    |            |   0% \u001b[0m\u001b[91m\n",
      "zlib-1.2.11          | 105 KB    | ########## | 100% \u001b[0m\u001b[91m\n",
      "\n",
      "libgcc-ng-9.1.0      | 8.1 MB    |            |   0% \u001b[0m\u001b[91m\n",
      "libgcc-ng-9.1.0      | 8.1 MB    | #####5     |  55% \u001b[0m\u001b[91m\n",
      "libgcc-ng-9.1.0      | 8.1 MB    | #######6   |  77% \u001b[0m\u001b[91m\n",
      "libgcc-ng-9.1.0      | 8.1 MB    | #########4 |  94% \u001b[0m\u001b[91m\n",
      "libgcc-ng-9.1.0      | 8.1 MB    | ########## | 100% \u001b[0m\u001b[91m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "xz-5.2.4             | 366 KB    |            |   0% \u001b[0m\u001b[91m\n",
      "xz-5.2.4             | 366 KB    | #########4 |  95% \u001b[0m\u001b[91m\n",
      "xz-5.2.4             | 366 KB    | ########## | 100% \u001b[0m\u001b[91m\n",
      "\n",
      "ncurses-5.9          | 1.1 MB    |            |   0% \u001b[0m\u001b[91m\n",
      "ncurses-5.9          | 1.1 MB    | #######8   |  79% \u001b[0m\u001b[91m\n",
      "ncurses-5.9          | 1.1 MB    | ########7  |  88% \u001b[0m\u001b[91m\n",
      "ncurses-5.9          | 1.1 MB    | #########7 |  97% \u001b[0m\u001b[91m\n",
      "ncurses-5.9          | 1.1 MB    | ########## | 100% \u001b[0m\n",
      "Downloading and Extracting Packages\n",
      "Preparing transaction: ...working... done\n",
      "Verifying transaction: ...working... done\n",
      "Executing transaction: ...working... done\n",
      "Collecting tensorflow-gpu==1.13.1\n",
      "  Downloading https://files.pythonhosted.org/packages/7b/b1/0ad4ae02e17ddd62109cd54c291e311c4b5fd09b4d0678d3d6ce4159b0f0/tensorflow_gpu-1.13.1-cp36-cp36m-manylinux1_x86_64.whl (345.2MB)\n",
      "Collecting azureml-defaults==1.0.69.*\n",
      "  Downloading https://files.pythonhosted.org/packages/1c/cb/5b70897d8c98daebcbba9df6a5fd4a5cf80ef52d97fa7ba7c6be5e6a2d69/azureml_defaults-1.0.69-py2.py3-none-any.whl\n",
      "Collecting astor>=0.6.0\n",
      "  Downloading https://files.pythonhosted.org/packages/d1/4f/950dfae467b384fc96bc6469de25d832534f6b4441033c39f914efd13418/astor-0.8.0-py2.py3-none-any.whl\n",
      "Collecting keras-preprocessing>=1.0.5\n",
      "  Downloading https://files.pythonhosted.org/packages/28/6a/8c1f62c37212d9fc441a7e26736df51ce6f0e38455816445471f10da4f0a/Keras_Preprocessing-1.1.0-py2.py3-none-any.whl (41kB)\n",
      "Collecting gast>=0.2.0\n",
      "  Downloading https://files.pythonhosted.org/packages/1f/04/4e36c33f8eb5c5b6c622a1f4859352a6acca7ab387257d4b3c191d23ec1d/gast-0.3.2.tar.gz\n",
      "Collecting termcolor>=1.1.0\n",
      "  Downloading https://files.pythonhosted.org/packages/8a/48/a76be51647d0eb9f10e2a4511bf3ffb8cc1e6b14e9e4fab46173aa79f981/termcolor-1.1.0.tar.gz\n",
      "Collecting numpy>=1.13.3\n",
      "  Downloading https://files.pythonhosted.org/packages/e5/e6/c3fdc53aed9fa19d6ff3abf97dfad768ae3afce1b7431f7500000816bda5/numpy-1.17.2-cp36-cp36m-manylinux1_x86_64.whl (20.4MB)\n",
      "Collecting absl-py>=0.1.6\n",
      "  Downloading https://files.pythonhosted.org/packages/3b/72/e6e483e2db953c11efa44ee21c5fdb6505c4dffa447b4263ca8af6676b62/absl-py-0.8.1.tar.gz (103kB)\n",
      "Collecting tensorboard<1.14.0,>=1.13.0\n",
      "  Downloading https://files.pythonhosted.org/packages/0f/39/bdd75b08a6fba41f098b6cb091b9e8c7a80e1b4d679a581a0ccd17b10373/tensorboard-1.13.1-py3-none-any.whl (3.2MB)\n",
      "Collecting six>=1.10.0\n",
      "  Downloading https://files.pythonhosted.org/packages/73/fb/00a976f728d0d1fecfe898238ce23f502a721c0ac0ecfedb80e0d88c64e9/six-1.12.0-py2.py3-none-any.whl\n",
      "Collecting keras-applications>=1.0.6\n",
      "  Downloading https://files.pythonhosted.org/packages/71/e3/19762fdfc62877ae9102edf6342d71b28fbfd9dea3d2f96a882ce099b03f/Keras_Applications-1.0.8-py3-none-any.whl (50kB)\n",
      "Collecting protobuf>=3.6.1\n",
      "  Downloading https://files.pythonhosted.org/packages/a8/52/d8d2dbff74b8bf517c42db8d44c3f9ef6555e6f5d6caddfa3f207b9143df/protobuf-3.10.0-cp36-cp36m-manylinux1_x86_64.whl (1.3MB)\n",
      "Collecting tensorflow-estimator<1.14.0rc0,>=1.13.0\n",
      "  Downloading https://files.pythonhosted.org/packages/bb/48/13f49fc3fa0fdf916aa1419013bb8f2ad09674c275b4046d5ee669a46873/tensorflow_estimator-1.13.0-py2.py3-none-any.whl (367kB)\n",
      "Requirement already satisfied: wheel>=0.26 in /azureml-envs/azureml_29ce53e4b6e031ec3cfd3ffd40786b73/lib/python3.6/site-packages (from tensorflow-gpu==1.13.1->-r /azureml-environment-setup/condaenv.ty1u4y30.requirements.txt (line 1)) (0.33.6)\n",
      "Collecting grpcio>=1.8.6\n",
      "  Downloading https://files.pythonhosted.org/packages/e8/cb/ebf7b54c5d4ad521d88ee7826dfa0fc3ac84502361ad7e5cb739ea5057a4/grpcio-1.24.1-cp36-cp36m-manylinux1_x86_64.whl (2.3MB)\n",
      "Collecting configparser==3.7.4\n",
      "  Downloading https://files.pythonhosted.org/packages/ba/05/6c96328e92e625fc31445d24d75a2c92ef9ba34fc5b037fe69693c362a0d/configparser-3.7.4-py2.py3-none-any.whl\n",
      "Collecting flask==1.0.3\n",
      "  Downloading https://files.pythonhosted.org/packages/9a/74/670ae9737d14114753b8c8fdf2e8bd212a05d3b361ab15b44937dfd40985/Flask-1.0.3-py2.py3-none-any.whl (92kB)\n",
      "Collecting azureml-core==1.0.69.*\n",
      "  Downloading https://files.pythonhosted.org/packages/9a/4f/b5c71c45f9aa82aa2636dd5ec7e19c6c11138c8ef127faa5cbbbc1efa0b7/azureml_core-1.0.69-py2.py3-none-any.whl (1.1MB)\n",
      "Collecting json-logging-py==0.2\n",
      "  Downloading https://files.pythonhosted.org/packages/e9/e1/46c70eebf216b830867c4896ee678cb7f1b28bb68a2810c7e9a811cecfbc/json-logging-py-0.2.tar.gz\n",
      "Collecting applicationinsights>=0.11.7\n",
      "  Downloading https://files.pythonhosted.org/packages/a1/53/234c53004f71f0717d8acd37876e0b65c121181167057b9ce1b1795f96a0/applicationinsights-0.11.9-py2.py3-none-any.whl (58kB)\n",
      "Collecting gunicorn==19.9.0\n",
      "  Downloading https://files.pythonhosted.org/packages/8c/da/b8dd8deb741bff556db53902d4706774c8e1e67265f69528c14c003644e6/gunicorn-19.9.0-py2.py3-none-any.whl (112kB)\n",
      "Collecting azureml-model-management-sdk==1.0.1b6.post1\n",
      "  Downloading https://files.pythonhosted.org/packages/4e/53/9004a1e7d6d4e796abc4bcc8286bfc2a32739c5fbac3859ca7429a228897/azureml_model_management_sdk-1.0.1b6.post1-py2.py3-none-any.whl (130kB)\n",
      "Collecting markdown>=2.6.8\n",
      "  Downloading https://files.pythonhosted.org/packages/c0/4e/fd492e91abdc2d2fcb70ef453064d980688762079397f779758e055f6575/Markdown-3.1.1-py2.py3-none-any.whl (87kB)\n",
      "Collecting werkzeug>=0.11.15\n",
      "  Downloading https://files.pythonhosted.org/packages/ce/42/3aeda98f96e85fd26180534d36570e4d18108d62ae36f87694b476b83d6f/Werkzeug-0.16.0-py2.py3-none-any.whl (327kB)\n",
      "Collecting h5py\n",
      "  Downloading https://files.pythonhosted.org/packages/60/06/cafdd44889200e5438b897388f3075b52a8ef01f28a17366d91de0fa2d05/h5py-2.10.0-cp36-cp36m-manylinux1_x86_64.whl (2.9MB)\n",
      "Requirement already satisfied: setuptools in /azureml-envs/azureml_29ce53e4b6e031ec3cfd3ffd40786b73/lib/python3.6/site-packages (from protobuf>=3.6.1->tensorflow-gpu==1.13.1->-r /azureml-environment-setup/condaenv.ty1u4y30.requirements.txt (line 1)) (41.4.0)\n",
      "Collecting mock>=2.0.0\n",
      "  Downloading https://files.pythonhosted.org/packages/05/d2/f94e68be6b17f46d2c353564da56e6fb89ef09faeeff3313a046cb810ca9/mock-3.0.5-py2.py3-none-any.whl\n",
      "Collecting click>=5.1\n",
      "  Downloading https://files.pythonhosted.org/packages/fa/37/45185cb5abbc30d7257104c434fe0b07e5a195a6847506c074527aa599ec/Click-7.0-py2.py3-none-any.whl (81kB)\n",
      "Collecting Jinja2>=2.10\n",
      "  Downloading https://files.pythonhosted.org/packages/65/e0/eb35e762802015cab1ccee04e8a277b03f1d8e53da3ec3106882ec42558b/Jinja2-2.10.3-py2.py3-none-any.whl (125kB)\n",
      "Collecting itsdangerous>=0.24\n",
      "  Downloading https://files.pythonhosted.org/packages/76/ae/44b03b253d6fade317f32c24d100b3b35c2239807046a4c953c7b89fa49e/itsdangerous-1.1.0-py2.py3-none-any.whl\n",
      "Collecting azure-mgmt-containerregistry>=2.0.0\n",
      "  Downloading https://files.pythonhosted.org/packages/97/70/8c2d0509db466678eba16fa2b0a539499f3b351b1f2993126ad843d5be13/azure_mgmt_containerregistry-2.8.0-py2.py3-none-any.whl (718kB)\n",
      "Collecting jsonpickle\n",
      "  Downloading https://files.pythonhosted.org/packages/07/07/c157520a3ebd166c8c24c6ae0ecae7c3968eb4653ff0e5af369bb82f004d/jsonpickle-1.2-py2.py3-none-any.whl\n",
      "Collecting azure-mgmt-authorization>=0.40.0\n",
      "  Downloading https://files.pythonhosted.org/packages/5e/17/4724694ddb3311955ddc367eddcd0928f8ee2c7b12d5a6f0b12bca0b03db/azure_mgmt_authorization-0.60.0-py2.py3-none-any.whl (82kB)\n",
      "Collecting pyopenssl\n",
      "  Downloading https://files.pythonhosted.org/packages/01/c8/ceb170d81bd3941cbeb9940fc6cc2ef2ca4288d0ca8929ea4db5905d904d/pyOpenSSL-19.0.0-py2.py3-none-any.whl (53kB)\n",
      "Collecting cryptography!=1.9,!=2.0.*,!=2.1.*,!=2.2.*\n",
      "  Downloading https://files.pythonhosted.org/packages/97/18/c6557f63a6abde34707196fb2cad1c6dc0dbff25a200d5044922496668a4/cryptography-2.7-cp34-abi3-manylinux1_x86_64.whl (2.3MB)\n",
      "Collecting urllib3>=1.23\n",
      "  Downloading https://files.pythonhosted.org/packages/e0/da/55f51ea951e1b7c63a579c09dd7db825bb730ec1fe9c0180fc77bfb31448/urllib3-1.25.6-py2.py3-none-any.whl (125kB)\n",
      "Collecting ndg-httpsclient\n",
      "  Downloading https://files.pythonhosted.org/packages/fb/67/c2f508c00ed2a6911541494504b7cac16fe0b0473912568df65fd1801132/ndg_httpsclient-0.5.1-py3-none-any.whl\n",
      "Collecting msrest>=0.5.1\n",
      "  Downloading https://files.pythonhosted.org/packages/27/b0/c34b3ea9b2ed74b800520fbefb312cdb7f05c20b8bd42e5e7662a5614f98/msrest-0.6.10-py2.py3-none-any.whl (82kB)\n",
      "Collecting azure-mgmt-storage>=1.5.0\n",
      "  Downloading https://files.pythonhosted.org/packages/a1/71/c1f73fa452b7f2e1b5567621a1386907cb4591ef8335c012ab4b358fb090/azure_mgmt_storage-4.2.0-py2.py3-none-any.whl (435kB)\n",
      "Collecting azure-graphrbac>=0.40.0\n",
      "  Downloading https://files.pythonhosted.org/packages/3e/93/02056aca45162f9fc275d1eaad12a2a07ef92375afb48eabddc4134b8315/azure_graphrbac-0.61.1-py2.py3-none-any.whl (141kB)\n",
      "Collecting adal>=1.2.0\n",
      "  Downloading https://files.pythonhosted.org/packages/4f/b5/3ea9ae3d1096b9ff31e8f1846c47d49f3129a12464ac0a73b602de458298/adal-1.2.2-py2.py3-none-any.whl (53kB)\n",
      "Collecting ruamel.yaml<=0.15.89,>=0.15.35\n",
      "  Downloading https://files.pythonhosted.org/packages/36/e1/cc2fa400fa5ffde3efa834ceb15c464075586de05ca3c553753dcd6f1d3b/ruamel.yaml-0.15.89-cp36-cp36m-manylinux1_x86_64.whl (651kB)\n",
      "Collecting azure-common>=1.1.12\n",
      "  Downloading https://files.pythonhosted.org/packages/00/55/a703923c12cd3172d5c007beda0c1a34342a17a6a72779f8a7c269af0cd6/azure_common-1.1.23-py2.py3-none-any.whl\n",
      "Collecting PyJWT\n",
      "  Downloading https://files.pythonhosted.org/packages/87/8b/6a9f14b5f781697e51259d81657e6048fd31a113229cf346880bb7545565/PyJWT-1.7.1-py2.py3-none-any.whl\n",
      "Collecting backports.tempfile\n",
      "  Downloading https://files.pythonhosted.org/packages/b4/5c/077f910632476281428fe254807952eb47ca78e720d059a46178c541e669/backports.tempfile-1.0-py2.py3-none-any.whl\n",
      "Collecting jmespath\n",
      "  Downloading https://files.pythonhosted.org/packages/83/94/7179c3832a6d45b266ddb2aac329e101367fbdb11f425f13771d27f225bb/jmespath-0.9.4-py2.py3-none-any.whl\n",
      "Collecting msrestazure>=0.4.33\n",
      "  Downloading https://files.pythonhosted.org/packages/68/75/5cb56ca8cbc6c5fe476e4878c73f57a331edcf55e5d3fcb4a7377d7d659d/msrestazure-0.6.2-py2.py3-none-any.whl (40kB)\n",
      "Collecting pytz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Downloading https://files.pythonhosted.org/packages/e7/f9/f0b53f88060247251bf481fa6ea62cd0d25bf1b11a87888e53ce5b7c8ad2/pytz-2019.3-py2.py3-none-any.whl (509kB)\n",
      "Collecting pathspec\n",
      "  Downloading https://files.pythonhosted.org/packages/7a/68/5902e8cd7f7b17c5879982a3a3ee2ad0c3b92b80c79989a2d3e1ca8d29e1/pathspec-0.6.0.tar.gz\n",
      "Collecting azure-mgmt-resource>=1.2.1\n",
      "  Downloading https://files.pythonhosted.org/packages/7c/0d/80815326fa04f2a73ea94b0f57c29669c89df5aa5f5e285952f6445a91c4/azure_mgmt_resource-5.1.0-py2.py3-none-any.whl (681kB)\n",
      "Collecting python-dateutil>=2.7.3\n",
      "  Downloading https://files.pythonhosted.org/packages/41/17/c62faccbfbd163c7f57f3844689e3a78bae1f403648a6afb1d0866d87fbb/python_dateutil-2.8.0-py2.py3-none-any.whl (226kB)\n",
      "Collecting requests>=2.19.1\n",
      "  Downloading https://files.pythonhosted.org/packages/51/bd/23c926cd341ea6b7dd0b2a00aba99ae0f828be89d72b2190f27c11d4b7fb/requests-2.22.0-py2.py3-none-any.whl (57kB)\n",
      "Collecting azure-mgmt-keyvault>=0.40.0\n",
      "  Downloading https://files.pythonhosted.org/packages/b3/d1/9fed0a3a3b43d0b1ad59599b5c836ccc4cf117e26458075385bafe79575b/azure_mgmt_keyvault-2.0.0-py2.py3-none-any.whl (80kB)\n",
      "Collecting SecretStorage\n",
      "  Downloading https://files.pythonhosted.org/packages/82/59/cb226752e20d83598d7fdcabd7819570b0329a61db07cfbdd21b2ef546e3/SecretStorage-3.1.1-py3-none-any.whl\n",
      "Collecting docker\n",
      "  Downloading https://files.pythonhosted.org/packages/cc/ca/699d4754a932787ef353a157ada74efd1ceb6d1fc0bfb7989ae1e7b33111/docker-4.1.0-py2.py3-none-any.whl (139kB)\n",
      "Collecting contextlib2\n",
      "  Downloading https://files.pythonhosted.org/packages/85/60/370352f7ef6aa96c52fb001831622f50f923c1d575427d021b8ab3311236/contextlib2-0.6.0.post1-py2.py3-none-any.whl\n",
      "Collecting dill>=0.2.7.1\n",
      "  Downloading https://files.pythonhosted.org/packages/c7/11/345f3173809cea7f1a193bfbf02403fff250a3360e0e118a1630985e547d/dill-0.3.1.1.tar.gz (151kB)\n",
      "Collecting liac-arff>=2.1.1\n",
      "  Downloading https://files.pythonhosted.org/packages/e9/35/fbc9217cfa91d98888b43e1a19c03a50d716108c58494c558c65e308f372/liac-arff-2.4.0.tar.gz\n",
      "Collecting pandas>=0.20.2\n",
      "  Downloading https://files.pythonhosted.org/packages/73/9b/52e228545d14f14bb2a1622e225f38463c8726645165e1cb7dde95bfe6d4/pandas-0.25.1-cp36-cp36m-manylinux1_x86_64.whl (10.5MB)\n",
      "Collecting MarkupSafe>=0.23\n",
      "  Downloading https://files.pythonhosted.org/packages/b2/5f/23e0023be6bb885d00ffbefad2942bc51a620328ee910f64abe5a8d18dd1/MarkupSafe-1.1.1-cp36-cp36m-manylinux1_x86_64.whl\n",
      "Collecting asn1crypto>=0.21.0\n",
      "  Downloading https://files.pythonhosted.org/packages/6b/8c/ff300484eca90b397d919408619fae479965bdd8a1df3d6d08d58e491da5/asn1crypto-1.2.0-py2.py3-none-any.whl (103kB)\n",
      "Collecting cffi!=1.11.3,>=1.8\n",
      "  Downloading https://files.pythonhosted.org/packages/6c/e6/2ca409305c8fb797a8ac65c8f5d4955ddea892bdf49e5319755207de1120/cffi-1.13.0-cp36-cp36m-manylinux1_x86_64.whl (430kB)\n",
      "Collecting pyasn1>=0.1.1\n",
      "  Downloading https://files.pythonhosted.org/packages/a1/71/8f0d444e3a74e5640a3d5d967c1c6b015da9c655f35b2d308a55d907a517/pyasn1-0.4.7-py2.py3-none-any.whl (76kB)\n",
      "Collecting isodate>=0.6.0\n",
      "  Downloading https://files.pythonhosted.org/packages/9b/9f/b36f7774ff5ea8e428fdcfc4bb332c39ee5b9362ddd3d40d9516a55221b2/isodate-0.6.0-py2.py3-none-any.whl (45kB)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /azureml-envs/azureml_29ce53e4b6e031ec3cfd3ffd40786b73/lib/python3.6/site-packages (from msrest>=0.5.1->azureml-core==1.0.69.*->azureml-defaults==1.0.69.*->-r /azureml-environment-setup/condaenv.ty1u4y30.requirements.txt (line 2)) (2019.9.11)\n",
      "Collecting requests-oauthlib>=0.5.0\n",
      "  Downloading https://files.pythonhosted.org/packages/c2/e2/9fd03d55ffb70fe51f587f20bcf407a6927eb121de86928b34d162f0b1ac/requests_oauthlib-1.2.0-py2.py3-none-any.whl\n",
      "Collecting backports.weakref\n",
      "  Downloading https://files.pythonhosted.org/packages/88/ec/f598b633c3d5ffe267aaada57d961c94fdfa183c5c3ebda2b6d151943db6/backports.weakref-1.0.post1-py2.py3-none-any.whl\n",
      "Collecting chardet<3.1.0,>=3.0.2\n",
      "  Downloading https://files.pythonhosted.org/packages/bc/a9/01ffebfb562e4274b6487b4bb1ddec7ca55ec7510b22e4c51f14098443b8/chardet-3.0.4-py2.py3-none-any.whl (133kB)\n",
      "Collecting idna<2.9,>=2.5\n",
      "  Downloading https://files.pythonhosted.org/packages/14/2c/cd551d81dbe15200be1cf41cd03869a46fe7226e7450af7a6545bfc474c9/idna-2.8-py2.py3-none-any.whl (58kB)\n",
      "Collecting jeepney\n",
      "  Downloading https://files.pythonhosted.org/packages/0a/4c/ef880713a6c6d628869596703167eab2edf8e0ec2d870d1089dcb0901b81/jeepney-0.4.1-py3-none-any.whl (60kB)\n",
      "Collecting websocket-client>=0.32.0\n",
      "  Downloading https://files.pythonhosted.org/packages/29/19/44753eab1fdb50770ac69605527e8859468f3c0fd7dc5a76dd9c4dbd7906/websocket_client-0.56.0-py2.py3-none-any.whl (200kB)\n",
      "Collecting pycparser\n",
      "  Downloading https://files.pythonhosted.org/packages/68/9e/49196946aee219aead1290e00d1e7fdeab8567783e83e1b9ab5585e6206a/pycparser-2.19.tar.gz (158kB)\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Downloading https://files.pythonhosted.org/packages/05/57/ce2e7a8fa7c0afb54a0581b14a65b56e62b5759dbc98e80627142b8a3704/oauthlib-3.1.0-py2.py3-none-any.whl (147kB)\n",
      "Building wheels for collected packages: gast, termcolor, absl-py, json-logging-py, pathspec, dill, liac-arff, pycparser\n",
      "  Building wheel for gast (setup.py): started\n",
      "  Building wheel for gast (setup.py): finished with status 'done'\n",
      "  Created wheel for gast: filename=gast-0.3.2-cp36-none-any.whl size=9679 sha256=51fb020dce418dfd45180281f293f6ac639f3712083625f7fc079d69242d4af3\n",
      "  Stored in directory: /root/.cache/pip/wheels/59/38/c6/234dc39b4f6951a0768fbc02d5b7207137a5b1d9094f0d54bf\n",
      "  Building wheel for termcolor (setup.py): started\n",
      "  Building wheel for termcolor (setup.py): finished with status 'done'\n",
      "  Created wheel for termcolor: filename=termcolor-1.1.0-cp36-none-any.whl size=4832 sha256=0aaf2801c0d25777311e1b3fff96acf3dd3a983130abe35c5564c3036cedd92a\n",
      "  Stored in directory: /root/.cache/pip/wheels/7c/06/54/bc84598ba1daf8f970247f550b175aaaee85f68b4b0c5ab2c6\n",
      "  Building wheel for absl-py (setup.py): started\n",
      "  Building wheel for absl-py (setup.py): finished with status 'done'\n",
      "  Created wheel for absl-py: filename=absl_py-0.8.1-cp36-none-any.whl size=121167 sha256=9ba4edab18a74c622bcab9f36d1d1a4364179c90672b05e1cd523c50952eed91\n",
      "  Stored in directory: /root/.cache/pip/wheels/a7/15/a0/0a0561549ad11cdc1bc8fa1191a353efd30facf6bfb507aefc\n",
      "  Building wheel for json-logging-py (setup.py): started\n",
      "  Building wheel for json-logging-py (setup.py): finished with status 'done'\n",
      "  Created wheel for json-logging-py: filename=json_logging_py-0.2-cp36-none-any.whl size=3924 sha256=08912eba4acd4d26e5c2168a3d29881bced58d8051505521d483cfb9edf6d319\n",
      "  Stored in directory: /root/.cache/pip/wheels/0d/2e/1c/c638b7589610d8b9358a6e5eb008edacb8b3e9b6d1edc9479f\n",
      "  Building wheel for pathspec (setup.py): started\n",
      "  Building wheel for pathspec (setup.py): finished with status 'done'\n",
      "  Created wheel for pathspec: filename=pathspec-0.6.0-cp36-none-any.whl size=26671 sha256=51b6735c7508f3892f4d377df7b040597ece3cd75a8d4f754561fab1e301db89\n",
      "  Stored in directory: /root/.cache/pip/wheels/62/b8/e1/e2719465b5947c40cd85d613d6cb33449b86a1ca5a6c574269\n",
      "  Building wheel for dill (setup.py): started\n",
      "  Building wheel for dill (setup.py): finished with status 'done'\n",
      "  Created wheel for dill: filename=dill-0.3.1.1-cp36-none-any.whl size=78532 sha256=e250dfc64ea6725c0b1ac098d097456455f60058b9fc7fbd7b8ca2ad4bd9768a\n",
      "  Stored in directory: /root/.cache/pip/wheels/59/b1/91/f02e76c732915c4015ab4010f3015469866c1eb9b14058d8e7\n",
      "  Building wheel for liac-arff (setup.py): started\n",
      "  Building wheel for liac-arff (setup.py): finished with status 'done'\n",
      "  Created wheel for liac-arff: filename=liac_arff-2.4.0-cp36-none-any.whl size=13333 sha256=10a2b3e276dc3ab481b92b41d159d99cf30aa2bd399a96aefbe593ed02bb0f43\n",
      "  Stored in directory: /root/.cache/pip/wheels/d1/6a/e7/529dc54d76ecede4346164a09ae3168df358945612710f5203\n",
      "  Building wheel for pycparser (setup.py): started\n",
      "  Building wheel for pycparser (setup.py): finished with status 'done'\n",
      "  Created wheel for pycparser: filename=pycparser-2.19-py2.py3-none-any.whl size=111029 sha256=a7c83315f2ee7d6b897b5ff008ebf2f2707f589c18202ada38507a15f292bd87\n",
      "  Stored in directory: /root/.cache/pip/wheels/f2/9a/90/de94f8556265ddc9d9c8b271b0f63e57b26fb1d67a45564511\n",
      "Successfully built gast termcolor absl-py json-logging-py pathspec dill liac-arff pycparser\n",
      "Installing collected packages: astor, six, numpy, keras-preprocessing, gast, termcolor, absl-py, grpcio, markdown, werkzeug, protobuf, tensorboard, h5py, keras-applications, mock, tensorflow-estimator, tensorflow-gpu, configparser, click, MarkupSafe, Jinja2, itsdangerous, flask, isodate, chardet, idna, urllib3, requests, oauthlib, requests-oauthlib, msrest, PyJWT, python-dateutil, asn1crypto, pycparser, cffi, cryptography, adal, msrestazure, azure-common, azure-mgmt-containerregistry, jsonpickle, azure-mgmt-authorization, pyopenssl, pyasn1, ndg-httpsclient, azure-mgmt-storage, azure-graphrbac, ruamel.yaml, backports.weakref, backports.tempfile, jmespath, pytz, pathspec, azure-mgmt-resource, azure-mgmt-keyvault, jeepney, SecretStorage, websocket-client, docker, contextlib2, azureml-core, json-logging-py, applicationinsights, gunicorn, dill, liac-arff, pandas, azureml-model-management-sdk, azureml-defaults\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully installed Jinja2-2.10.3 MarkupSafe-1.1.1 PyJWT-1.7.1 SecretStorage-3.1.1 absl-py-0.8.1 adal-1.2.2 applicationinsights-0.11.9 asn1crypto-1.2.0 astor-0.8.0 azure-common-1.1.23 azure-graphrbac-0.61.1 azure-mgmt-authorization-0.60.0 azure-mgmt-containerregistry-2.8.0 azure-mgmt-keyvault-2.0.0 azure-mgmt-resource-5.1.0 azure-mgmt-storage-4.2.0 azureml-core-1.0.69 azureml-defaults-1.0.69 azureml-model-management-sdk-1.0.1b6.post1 backports.tempfile-1.0 backports.weakref-1.0.post1 cffi-1.13.0 chardet-3.0.4 click-7.0 configparser-3.7.4 contextlib2-0.6.0.post1 cryptography-2.7 dill-0.3.1.1 docker-4.1.0 flask-1.0.3 gast-0.3.2 grpcio-1.24.1 gunicorn-19.9.0 h5py-2.10.0 idna-2.8 isodate-0.6.0 itsdangerous-1.1.0 jeepney-0.4.1 jmespath-0.9.4 json-logging-py-0.2 jsonpickle-1.2 keras-applications-1.0.8 keras-preprocessing-1.1.0 liac-arff-2.4.0 markdown-3.1.1 mock-3.0.5 msrest-0.6.10 msrestazure-0.6.2 ndg-httpsclient-0.5.1 numpy-1.17.2 oauthlib-3.1.0 pandas-0.25.1 pathspec-0.6.0 protobuf-3.10.0 pyasn1-0.4.7 pycparser-2.19 pyopenssl-19.0.0 python-dateutil-2.8.0 pytz-2019.3 requests-2.22.0 requests-oauthlib-1.2.0 ruamel.yaml-0.15.89 six-1.12.0 tensorboard-1.13.1 tensorflow-estimator-1.13.0 tensorflow-gpu-1.13.1 termcolor-1.1.0 urllib3-1.25.6 websocket-client-0.56.0 werkzeug-0.16.0\n",
      "\u001b[91m\n",
      "\u001b[0m#\n",
      "# To activate this environment, use:\n",
      "# > source activate /azureml-envs/azureml_29ce53e4b6e031ec3cfd3ffd40786b73\n",
      "#\n",
      "# To deactivate an active environment, use:\n",
      "# > source deactivate\n",
      "#\n",
      "\n",
      "Removing intermediate container ddc22ca2f45c\n",
      " ---> 5fc04e9591ba\n",
      "Step 9/14 : ENV PATH /azureml-envs/azureml_29ce53e4b6e031ec3cfd3ffd40786b73/bin:$PATH\n",
      " ---> Running in cd8a510a0ba3\n",
      "Removing intermediate container cd8a510a0ba3\n",
      " ---> 0f1783bbd459\n",
      "Step 10/14 : ENV AZUREML_CONDA_ENVIRONMENT_PATH /azureml-envs/azureml_29ce53e4b6e031ec3cfd3ffd40786b73\n",
      " ---> Running in 9c72609240d0\n",
      "Removing intermediate container 9c72609240d0\n",
      " ---> 90536c2a32e0\n",
      "Step 11/14 : ENV LD_LIBRARY_PATH /azureml-envs/azureml_29ce53e4b6e031ec3cfd3ffd40786b73/lib:$LD_LIBRARY_PATH\n",
      " ---> Running in a9eb3e922f3f\n",
      "Removing intermediate container a9eb3e922f3f\n",
      " ---> 8f8c82b03a83\n",
      "Step 12/14 : COPY azureml-environment-setup/spark_cache.py azureml-environment-setup/log4j.properties /azureml-environment-setup/\n",
      " ---> bc4549ee7155\n",
      "Step 13/14 : ENV AZUREML_ENVIRONMENT_IMAGE True\n",
      " ---> Running in dff3dc1cec6e\n",
      "Removing intermediate container dff3dc1cec6e\n",
      " ---> c0fbf644c67a\n",
      "Step 14/14 : CMD [\"bash\"]\n",
      " ---> Running in a6dac01633b7\n",
      "Removing intermediate container a6dac01633b7\n",
      " ---> 48a65db1d020\n",
      "Successfully built 48a65db1d020\n",
      "Successfully tagged mlservicedem1fffd9c8.azurecr.io/azureml/azureml_e7c5a562d687207f4fd718d55c21dc68:latest\n",
      "2019/10/17 11:15:40 Successfully executed container: acb_step_0\n",
      "2019/10/17 11:15:40 Executing step ID: acb_step_1. Timeout(sec): 5400, Working directory: '', Network: 'acb_default_network'\n",
      "2019/10/17 11:15:40 Pushing image: mlservicedem1fffd9c8.azurecr.io/azureml/azureml_e7c5a562d687207f4fd718d55c21dc68:latest, attempt 1\n",
      "The push refers to repository [mlservicedem1fffd9c8.azurecr.io/azureml/azureml_e7c5a562d687207f4fd718d55c21dc68]\n",
      "4aaf6eb8c34c: Preparing\n",
      "2b9cdaa542d4: Preparing\n",
      "88dee2014ee7: Preparing\n",
      "36e2ae3db6b1: Preparing\n",
      "b0e801c576e8: Preparing\n",
      "a507035a7b55: Preparing\n",
      "a433ba97e2c6: Preparing\n",
      "e1347ef8649c: Preparing\n",
      "115b48a7063d: Preparing\n",
      "8269430f5609: Preparing\n",
      "98ee984ecda9: Preparing\n",
      "89a858fcdb56: Preparing\n",
      "1d7438ff2020: Preparing\n",
      "c36fc92de581: Preparing\n",
      "524881d0563d: Preparing\n",
      "d05c249e6b9f: Preparing\n",
      "fc8a172f94e8: Preparing\n",
      "48001c8a7ddb: Preparing\n",
      "6dce9683cb41: Preparing\n",
      "e79142719515: Preparing\n",
      "aeda103e78c9: Preparing\n",
      "2558e637fbff: Preparing\n",
      "f749b9b0fb21: Preparing\n",
      "a507035a7b55: Waiting\n",
      "a433ba97e2c6: Waiting\n",
      "e1347ef8649c: Waiting\n",
      "115b48a7063d: Waiting\n",
      "8269430f5609: Waiting\n",
      "98ee984ecda9: Waiting\n",
      "89a858fcdb56: Waiting\n",
      "1d7438ff2020: Waiting\n",
      "c36fc92de581: Waiting\n",
      "524881d0563d: Waiting\n",
      "d05c249e6b9f: Waiting\n",
      "fc8a172f94e8: Waiting\n",
      "48001c8a7ddb: Waiting\n",
      "6dce9683cb41: Waiting\n",
      "e79142719515: Waiting\n",
      "aeda103e78c9: Waiting\n",
      "2558e637fbff: Waiting\n",
      "f749b9b0fb21: Waiting\n",
      "4aaf6eb8c34c: Pushed\n",
      "88dee2014ee7: Pushed\n",
      "b0e801c576e8: Pushed\n",
      "36e2ae3db6b1: Pushed\n",
      "a507035a7b55: Pushed\n",
      "a433ba97e2c6: Pushed\n",
      "e1347ef8649c: Pushed\n",
      "89a858fcdb56: Pushed\n",
      "98ee984ecda9: Pushed\n",
      "8269430f5609: Pushed\n",
      "1d7438ff2020: Pushed\n",
      "115b48a7063d: Pushed\n",
      "fc8a172f94e8: Pushed\n",
      "48001c8a7ddb: Pushed\n",
      "6dce9683cb41: Pushed\n",
      "e79142719515: Pushed\n",
      "aeda103e78c9: Pushed\n",
      "c36fc92de581: Pushed\n",
      "2558e637fbff: Pushed\n",
      "d05c249e6b9f: Pushed\n",
      "f749b9b0fb21: Pushed\n",
      "2b9cdaa542d4: Pushed\n",
      "524881d0563d: Pushed\n",
      "latest: digest: sha256:e0965b339dde23d0ddc1ccfebf9f1dbcab252d4ec637187e9e382c7f96db9470 size: 5151\n",
      "2019/10/17 11:21:29 Successfully pushed image: mlservicedem1fffd9c8.azurecr.io/azureml/azureml_e7c5a562d687207f4fd718d55c21dc68:latest\n",
      "2019/10/17 11:21:29 Step ID: acb_step_0 marked as successful (elapsed time in seconds: 241.863821)\n",
      "2019/10/17 11:21:29 Populating digests for step ID: acb_step_0...\n",
      "2019/10/17 11:21:30 Successfully populated digests for step ID: acb_step_0\n",
      "2019/10/17 11:21:30 Step ID: acb_step_1 marked as successful (elapsed time in seconds: 348.512480)\n",
      "2019/10/17 11:21:30 The following dependencies were found:\n",
      "2019/10/17 11:21:30 \n",
      "- image:\n",
      "    registry: mlservicedem1fffd9c8.azurecr.io\n",
      "    repository: azureml/azureml_e7c5a562d687207f4fd718d55c21dc68\n",
      "    tag: latest\n",
      "    digest: sha256:e0965b339dde23d0ddc1ccfebf9f1dbcab252d4ec637187e9e382c7f96db9470\n",
      "  runtime-dependency:\n",
      "    registry: mcr.microsoft.com\n",
      "    repository: azureml/base-gpu\n",
      "    tag: intelmpi2018.3-cuda10.0-cudnn7-ubuntu16.04\n",
      "    digest: sha256:b747b5a1dbdb93aea632b14764219c834dae19e09433fb29729b67d05548756e\n",
      "  git: {}\n",
      "\n",
      "\n",
      "Run ID: cbb was successful after 9m57s\n",
      "\n",
      "Streaming azureml-logs/70_driver_log.txt\n",
      "========================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING - Retrying (Retry(total=2, connect=3, read=2, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', OSError(\"(10060, 'WSAETIMEDOUT')\"))': /azureml/ExperimentRun/dcid.bbc67064-59c5-4fa1-8baa-fec778ff3f33/azureml-logs/70_driver_log.txt?sv=2018-11-09&sr=b&sig=HoZ4c2AXUIXX%2BHK76f9Z%2FgSIzV6iBmfqOS%2BttM7nZA8%3D&st=2019-10-17T11%3A16%3A36Z&se=2019-10-17T19%3A26%3A36Z&sp=r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bash: /azureml-envs/azureml_29ce53e4b6e031ec3cfd3ffd40786b73/lib/libtinfo.so.5: no version information available (required by bash)\n",
      "bash: /azureml-envs/azureml_29ce53e4b6e031ec3cfd3ffd40786b73/lib/libtinfo.so.5: no version information available (required by bash)\n",
      "Starting the daemon thread to refresh tokens in background for process with pid = 208\n",
      "Entering Run History Context Manager.\n",
      "/azureml-envs/azureml_29ce53e4b6e031ec3cfd3ffd40786b73/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/azureml-envs/azureml_29ce53e4b6e031ec3cfd3ffd40786b73/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/azureml-envs/azureml_29ce53e4b6e031ec3cfd3ffd40786b73/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/azureml-envs/azureml_29ce53e4b6e031ec3cfd3ffd40786b73/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/azureml-envs/azureml_29ce53e4b6e031ec3cfd3ffd40786b73/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/azureml-envs/azureml_29ce53e4b6e031ec3cfd3ffd40786b73/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "2019-10-17 11:26:19.507344: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "2019-10-17 11:26:19.881608: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x4f6c910 executing computations on platform CUDA. Devices:\n",
      "2019-10-17 11:26:19.881696: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): Tesla K80, Compute Capability 3.7\n",
      "2019-10-17 11:26:19.884495: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2596995000 Hz\n",
      "2019-10-17 11:26:19.884880: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x4fd4ba0 executing computations on platform Host. Devices:\n",
      "2019-10-17 11:26:19.884979: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>\n",
      "2019-10-17 11:26:19.885178: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1433] Found device 0 with properties: \n",
      "name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\n",
      "pciBusID: bee5:00:00.0\n",
      "totalMemory: 11.17GiB freeMemory: 11.11GiB\n",
      "2019-10-17 11:26:19.885236: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1512] Adding visible gpu devices: 0\n",
      "2019-10-17 11:26:19.887927: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2019-10-17 11:26:19.887999: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990]      0 \n",
      "2019-10-17 11:26:19.888057: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1003] 0:   N \n",
      "2019-10-17 11:26:19.888187: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10805 MB memory) -> physical GPU (device: 0, name: Tesla K80, pci bus id: bee5:00:00.0, compute capability: 3.7)\n",
      "WARNING:tensorflow:From batch_scoring.py:54: slice_input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensor_slices(tuple(tensor_list)).shuffle(tf.shape(input_tensor, out_type=tf.int64)[0]).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`.\n",
      "WARNING:tensorflow:From /azureml-envs/azureml_29ce53e4b6e031ec3cfd3ffd40786b73/lib/python3.6/site-packages/tensorflow/python/training/input.py:374: range_input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.range(limit).shuffle(limit).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`.\n",
      "WARNING:tensorflow:From /azureml-envs/azureml_29ce53e4b6e031ec3cfd3ffd40786b73/lib/python3.6/site-packages/tensorflow/python/training/input.py:320: input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensor_slices(input_tensor).shuffle(tf.shape(input_tensor, out_type=tf.int64)[0]).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`.\n",
      "WARNING:tensorflow:From /azureml-envs/azureml_29ce53e4b6e031ec3cfd3ffd40786b73/lib/python3.6/site-packages/tensorflow/python/training/input.py:190: limit_epochs (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensors(tensor).repeat(num_epochs)`.\n",
      "WARNING:tensorflow:From /azureml-envs/azureml_29ce53e4b6e031ec3cfd3ffd40786b73/lib/python3.6/site-packages/tensorflow/python/training/input.py:199: QueueRunner.__init__ (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n",
      "WARNING:tensorflow:From /azureml-envs/azureml_29ce53e4b6e031ec3cfd3ffd40786b73/lib/python3.6/site-packages/tensorflow/python/training/input.py:199: add_queue_runner (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n",
      "WARNING:tensorflow:From /azureml-envs/azureml_29ce53e4b6e031ec3cfd3ffd40786b73/lib/python3.6/site-packages/tensorflow/python/training/input.py:202: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From batch_scoring.py:64: batch (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.batch(batch_size)` (or `padded_batch(...)` if `dynamic_pad=True`).\n",
      "WARNING:tensorflow:From /azureml-envs/azureml_29ce53e4b6e031ec3cfd3ffd40786b73/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From batch_scoring.py:91: start_queue_runners (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n",
      "WARNING:tensorflow:From /azureml-envs/azureml_29ce53e4b6e031ec3cfd3ffd40786b73/lib/python3.6/site-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n",
      "\n",
      "\n",
      "The experiment completed successfully. Finalizing run...\n",
      "Cleaning up all outstanding Run operations, waiting 300.0 seconds\n",
      "2 items cleaning up...\n",
      "Cleanup took 0.0017135143280029297 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Streaming azureml-logs/75_job_post-tvmps_b4f762f185e395226d5927fe9cb3656c8ca23306699d01467ef0693ef70b9315_d.txt\n",
      "===============================================================================================================\n",
      "bash: /azureml-envs/azureml_29ce53e4b6e031ec3cfd3ffd40786b73/lib/libtinfo.so.5: no version information available (required by bash)\n",
      "Starting job release. Current time:2019-10-17T11:26:43.800756\n",
      "Logging experiment finalizing status in history service.\n",
      "Starting the daemon thread to refresh tokens in background for process with pid = 316\n",
      "Job release is complete. Current time:2019-10-17T11:26:45.525834\n",
      "\n",
      "StepRun(batch_scoring) Execution Summary\n",
      "=========================================\n",
      "StepRun( batch_scoring ) Status: Finished\n",
      "{'runId': 'bbc67064-59c5-4fa1-8baa-fec778ff3f33', 'target': 'gpu-cluster', 'status': 'Completed', 'startTimeUtc': '2019-10-17T11:23:38.944763Z', 'endTimeUtc': '2019-10-17T11:26:58.557844Z', 'properties': {'azureml.runsource': 'azureml.StepRun', 'ContentSnapshotId': 'adb4db70-c6db-418a-8dd4-1718b1f1c17d', 'StepType': 'PythonScriptStep', 'ComputeTargetType': 'AmlCompute', 'azureml.pipelinerunid': '07abfe6a-33ed-4f20-83ef-ba82ce11727a', '_azureml.ComputeTargetType': 'batchai', 'AzureML.DerivedImageName': 'azureml/azureml_e7c5a562d687207f4fd718d55c21dc68', 'ProcessInfoFile': 'azureml-logs/process_info.json', 'ProcessStatusFile': 'azureml-logs/process_status.json'}, 'inputDatasets': [], 'runDefinition': {'script': 'batch_scoring.py', 'arguments': ['--dataset_path', '$AZUREML_DATAREFERENCE_input_images', '--model_name', 'inception', '--label_dir', '$AZUREML_DATAREFERENCE_input_labels', '--output_dir', '$AZUREML_DATAREFERENCE_scores', '--batch_size', '$AML_PARAMETER_param_batch_size'], 'sourceDirectoryDataStore': None, 'framework': 'Python', 'communicator': 'None', 'target': 'gpu-cluster', 'dataReferences': {'input_images': {'dataStoreName': 'images_datastore', 'mode': 'Download', 'pathOnDataStore': 'batchscoring/images', 'pathOnCompute': None, 'overwrite': False}, 'input_labels': {'dataStoreName': 'images_datastore', 'mode': 'Download', 'pathOnDataStore': 'batchscoring/labels', 'pathOnCompute': None, 'overwrite': False}, 'scores': {'dataStoreName': 'workspaceblobstore', 'mode': 'Mount', 'pathOnDataStore': 'azureml/bbc67064-59c5-4fa1-8baa-fec778ff3f33/scores', 'pathOnCompute': 'batchscoring/results', 'overwrite': False}}, 'data': {}, 'jobName': None, 'maxRunDurationSeconds': None, 'nodeCount': 1, 'environment': {'name': 'Experiment batch_scoring Environment', 'version': 'Autosave_2019-10-17T11:11:27Z_2eea1895', 'python': {'interpreterPath': 'python', 'userManagedDependencies': False, 'condaDependencies': {'channels': ['conda-forge'], 'dependencies': ['python=3.6.2', {'pip': ['tensorflow-gpu==1.13.1', 'azureml-defaults==1.0.69.*']}], 'name': 'azureml_29ce53e4b6e031ec3cfd3ffd40786b73'}, 'baseCondaEnvironment': None}, 'environmentVariables': {'EXAMPLE_ENV_VAR': 'EXAMPLE_VALUE', 'AML_PARAMETER_param_batch_size': '20'}, 'docker': {'baseImage': 'mcr.microsoft.com/azureml/base-gpu:intelmpi2018.3-cuda10.0-cudnn7-ubuntu16.04', 'baseDockerfile': None, 'baseImageRegistry': {'address': None, 'username': None, 'password': None}, 'enabled': True, 'shmSize': '1g'}, 'spark': {'repositories': ['[]'], 'packages': [], 'precachePackages': False}, 'inferencingStackVersion': None}, 'history': {'outputCollection': True, 'directoriesToWatch': ['logs']}, 'spark': {'configuration': {'spark.app.name': 'Azure ML Experiment', 'spark.yarn.maxAppAttempts': '1'}}, 'amlCompute': {'name': None, 'vmSize': None, 'retainCluster': False, 'clusterMaxNodeCount': 1}, 'tensorflow': {'workerCount': 1, 'parameterServerCount': 1}, 'mpi': {'processCountPerNode': 1}, 'hdi': {'yarnDeployMode': 'Cluster'}, 'containerInstance': {'region': None, 'cpuCores': 2, 'memoryGb': 3.5}, 'exposedPorts': None, 'docker': {'useDocker': True, 'sharedVolumes': True, 'shmSize': '1g', 'arguments': []}}, 'logFiles': {'azureml-logs/20_image_build_log.txt': 'https://mlservicedemo9711458422.blob.core.windows.net/azureml/ExperimentRun/dcid.bbc67064-59c5-4fa1-8baa-fec778ff3f33/azureml-logs/20_image_build_log.txt?sv=2018-11-09&sr=b&sig=j3EvwEd5h230eQT5TvLOYkumNWz9y4NrtzVxmF15oas%3D&st=2019-10-17T11%3A17%3A03Z&se=2019-10-17T19%3A27%3A03Z&sp=r', 'azureml-logs/55_azureml-execution-tvmps_b4f762f185e395226d5927fe9cb3656c8ca23306699d01467ef0693ef70b9315_d.txt': 'https://mlservicedemo9711458422.blob.core.windows.net/azureml/ExperimentRun/dcid.bbc67064-59c5-4fa1-8baa-fec778ff3f33/azureml-logs/55_azureml-execution-tvmps_b4f762f185e395226d5927fe9cb3656c8ca23306699d01467ef0693ef70b9315_d.txt?sv=2018-11-09&sr=b&sig=FKng0OHunJR%2BkYZwMp9Aoy5OmC%2F0NDBLEg%2BbilXzVdo%3D&st=2019-10-17T11%3A17%3A03Z&se=2019-10-17T19%3A27%3A03Z&sp=r', 'azureml-logs/65_job_prep-tvmps_b4f762f185e395226d5927fe9cb3656c8ca23306699d01467ef0693ef70b9315_d.txt': 'https://mlservicedemo9711458422.blob.core.windows.net/azureml/ExperimentRun/dcid.bbc67064-59c5-4fa1-8baa-fec778ff3f33/azureml-logs/65_job_prep-tvmps_b4f762f185e395226d5927fe9cb3656c8ca23306699d01467ef0693ef70b9315_d.txt?sv=2018-11-09&sr=b&sig=29RTqoFeBGaQ5Fi5jCxltuk2l45JbHtqk5a7mbHwSC8%3D&st=2019-10-17T11%3A17%3A03Z&se=2019-10-17T19%3A27%3A03Z&sp=r', 'azureml-logs/70_driver_log.txt': 'https://mlservicedemo9711458422.blob.core.windows.net/azureml/ExperimentRun/dcid.bbc67064-59c5-4fa1-8baa-fec778ff3f33/azureml-logs/70_driver_log.txt?sv=2018-11-09&sr=b&sig=JSTmg3BjJq3UqPXUv5iu0bMWe6D26G5JMcSdmC5m9EE%3D&st=2019-10-17T11%3A17%3A03Z&se=2019-10-17T19%3A27%3A03Z&sp=r', 'azureml-logs/75_job_post-tvmps_b4f762f185e395226d5927fe9cb3656c8ca23306699d01467ef0693ef70b9315_d.txt': 'https://mlservicedemo9711458422.blob.core.windows.net/azureml/ExperimentRun/dcid.bbc67064-59c5-4fa1-8baa-fec778ff3f33/azureml-logs/75_job_post-tvmps_b4f762f185e395226d5927fe9cb3656c8ca23306699d01467ef0693ef70b9315_d.txt?sv=2018-11-09&sr=b&sig=tadLIwG9XGf1%2BLfG2EoBg0yagsMv7nxIgbSKJXGAnpk%3D&st=2019-10-17T11%3A17%3A03Z&se=2019-10-17T19%3A27%3A03Z&sp=r', 'logs/azureml/208_azureml.log': 'https://mlservicedemo9711458422.blob.core.windows.net/azureml/ExperimentRun/dcid.bbc67064-59c5-4fa1-8baa-fec778ff3f33/logs/azureml/208_azureml.log?sv=2018-11-09&sr=b&sig=pgPEjg0Mp9ABhBpMEIfCVAOwHHWnVLVTl6ZH23ZHqBk%3D&st=2019-10-17T11%3A17%3A03Z&se=2019-10-17T19%3A27%3A03Z&sp=r', 'logs/azureml/azureml.log': 'https://mlservicedemo9711458422.blob.core.windows.net/azureml/ExperimentRun/dcid.bbc67064-59c5-4fa1-8baa-fec778ff3f33/logs/azureml/azureml.log?sv=2018-11-09&sr=b&sig=oMV3eT%2BXrCQFLB96%2B%2B8ZRUgtQJh9jxOHs7v20wBDqJg%3D&st=2019-10-17T11%3A17%3A03Z&se=2019-10-17T19%3A27%3A03Z&sp=r', 'logs/azureml/executionlogs.txt': 'https://mlservicedemo9711458422.blob.core.windows.net/azureml/ExperimentRun/dcid.bbc67064-59c5-4fa1-8baa-fec778ff3f33/logs/azureml/executionlogs.txt?sv=2018-11-09&sr=b&sig=p1nZR3egvDROsNFYGxhenKdUg%2BtNLLSmpFl5sBVEeRU%3D&st=2019-10-17T11%3A17%3A03Z&se=2019-10-17T19%3A27%3A03Z&sp=r', 'logs/azureml/stderrlogs.txt': 'https://mlservicedemo9711458422.blob.core.windows.net/azureml/ExperimentRun/dcid.bbc67064-59c5-4fa1-8baa-fec778ff3f33/logs/azureml/stderrlogs.txt?sv=2018-11-09&sr=b&sig=JcRuyIVvqky3yewNr%2F4DAejWjw6YvpJvnTOWCr%2BDeTU%3D&st=2019-10-17T11%3A17%3A03Z&se=2019-10-17T19%3A27%3A03Z&sp=r', 'logs/azureml/stdoutlogs.txt': 'https://mlservicedemo9711458422.blob.core.windows.net/azureml/ExperimentRun/dcid.bbc67064-59c5-4fa1-8baa-fec778ff3f33/logs/azureml/stdoutlogs.txt?sv=2018-11-09&sr=b&sig=439J%2FNwLVpWvY2SWbTdf0bp2cirf4MlvnOuxisgOR7I%3D&st=2019-10-17T11%3A17%3A03Z&se=2019-10-17T19%3A27%3A03Z&sp=r'}}\n",
      "\n",
      "\n",
      "\n",
      "PipelineRun Execution Summary\n",
      "==============================\n",
      "PipelineRun Status: Finished\n",
      "{'runId': '07abfe6a-33ed-4f20-83ef-ba82ce11727a', 'status': 'Completed', 'startTimeUtc': '2019-10-17T11:11:16.323561Z', 'endTimeUtc': '2019-10-17T11:27:03.063157Z', 'properties': {'azureml.runsource': 'azureml.PipelineRun', 'runSource': None, 'runType': 'HTTP', 'azureml.parameters': '{\"param_batch_size\":\"20\"}'}, 'inputDatasets': [], 'logFiles': {'logs/azureml/executionlogs.txt': 'https://mlservicedemo9711458422.blob.core.windows.net/azureml/ExperimentRun/dcid.07abfe6a-33ed-4f20-83ef-ba82ce11727a/logs/azureml/executionlogs.txt?sv=2018-11-09&sr=b&sig=Voy3L0Z6Edqew0cRiNPeop3lnEr7OLwrNRp5B8KWXVc%3D&st=2019-10-17T11%3A17%3A05Z&se=2019-10-17T19%3A27%3A05Z&sp=r', 'logs/azureml/stderrlogs.txt': 'https://mlservicedemo9711458422.blob.core.windows.net/azureml/ExperimentRun/dcid.07abfe6a-33ed-4f20-83ef-ba82ce11727a/logs/azureml/stderrlogs.txt?sv=2018-11-09&sr=b&sig=vRgIBYJX0QxJqHL077aaYK%2FahyMv%2FD%2B8oS9PQRIXTo4%3D&st=2019-10-17T11%3A17%3A05Z&se=2019-10-17T19%3A27%3A05Z&sp=r', 'logs/azureml/stdoutlogs.txt': 'https://mlservicedemo9711458422.blob.core.windows.net/azureml/ExperimentRun/dcid.07abfe6a-33ed-4f20-83ef-ba82ce11727a/logs/azureml/stdoutlogs.txt?sv=2018-11-09&sr=b&sig=Ur2lFrwpchb%2BhDaY6E90XUq3Ld3hu5HBJuvgvsqZaeE%3D&st=2019-10-17T11%3A17%3A05Z&se=2019-10-17T19%3A27%3A05Z&sp=r'}}\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Finished'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from azureml.core import Experiment\n",
    "from azureml.pipeline.core import Pipeline\n",
    "\n",
    "pipeline = Pipeline(workspace=ws, steps=[batch_score_step])\n",
    "pipeline_run = Experiment(ws, 'batch_scoring').submit(pipeline, pipeline_parameters={\"param_batch_size\": 20})\n",
    "pipeline_run.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download and review output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following code to download the output file created from the `batch_scoring.py` script, then explore the scoring results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Filename</th>\n",
       "      <th>Prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ILSVRC2012_val_00000623.JPEG</td>\n",
       "      <td>submarine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ILSVRC2012_val_00000157.JPEG</td>\n",
       "      <td>red fox</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ILSVRC2012_val_00000921.JPEG</td>\n",
       "      <td>electric ray</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ILSVRC2012_val_00000575.JPEG</td>\n",
       "      <td>hook</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ILSVRC2012_val_00000204.JPEG</td>\n",
       "      <td>stage</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ILSVRC2012_val_00000867.JPEG</td>\n",
       "      <td>miniature poodle</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ILSVRC2012_val_00000102.JPEG</td>\n",
       "      <td>Rhodesian ridgeback</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ILSVRC2012_val_00000911.JPEG</td>\n",
       "      <td>hen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ILSVRC2012_val_00000292.JPEG</td>\n",
       "      <td>thresher</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ILSVRC2012_val_00000352.JPEG</td>\n",
       "      <td>car mirror</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       Filename            Prediction\n",
       "0  ILSVRC2012_val_00000623.JPEG             submarine\n",
       "1  ILSVRC2012_val_00000157.JPEG               red fox\n",
       "2  ILSVRC2012_val_00000921.JPEG          electric ray\n",
       "3  ILSVRC2012_val_00000575.JPEG                  hook\n",
       "4  ILSVRC2012_val_00000204.JPEG                 stage\n",
       "5  ILSVRC2012_val_00000867.JPEG      miniature poodle\n",
       "6  ILSVRC2012_val_00000102.JPEG   Rhodesian ridgeback\n",
       "7  ILSVRC2012_val_00000911.JPEG                   hen\n",
       "8  ILSVRC2012_val_00000292.JPEG              thresher\n",
       "9  ILSVRC2012_val_00000352.JPEG            car mirror"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "step_run = list(pipeline_run.get_children())[0]\n",
    "step_run.download_file(\"./outputs/result-labels.txt\")\n",
    "\n",
    "df = pd.read_csv(\"result-labels.txt\", delimiter=\":\", header=None)\n",
    "df.columns = [\"Filename\", \"Prediction\"]\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Publish and run from REST endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following code to publish the pipeline to your workspace. In your workspace in the portal, you can see metadata for the pipeline including run history and durations. You can also run the pipeline manually from the portal.\n",
    "\n",
    "Additionally, publishing the pipeline enables a REST endpoint to rerun the pipeline from any HTTP library on any platform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"width:100%\"><tr><th>Name</th><th>Id</th><th>Status</th><th>Endpoint</th></tr><tr><td>Inception_v3_scoring</td><td><a href=\"https://mlworkspace.azure.ai/portal/subscriptions/7d48758f-d40b-4252-854c-e7d8f2ed7645/resourceGroups/AzureMLDemo/providers/Microsoft.MachineLearningServices/workspaces/ML_Service_Demo/pipelines/ddbf8582-5f16-4889-ba27-36379028eb20\" target=\"_blank\" rel=\"noopener\">ddbf8582-5f16-4889-ba27-36379028eb20</a></td><td>Active</td><td><a href=\"https://westeurope.aether.ms/api/v1.0/subscriptions/7d48758f-d40b-4252-854c-e7d8f2ed7645/resourceGroups/AzureMLDemo/providers/Microsoft.MachineLearningServices/workspaces/ML_Service_Demo/PipelineRuns/PipelineSubmit/ddbf8582-5f16-4889-ba27-36379028eb20\" target=\"_blank\" rel=\"noopener\">REST Endpoint</a></td></tr></table>"
      ],
      "text/plain": [
       "Pipeline(Name: Inception_v3_scoring,\n",
       "Id: ddbf8582-5f16-4889-ba27-36379028eb20,\n",
       "Status: Active,\n",
       "Endpoint: https://westeurope.aether.ms/api/v1.0/subscriptions/7d48758f-d40b-4252-854c-e7d8f2ed7645/resourceGroups/AzureMLDemo/providers/Microsoft.MachineLearningServices/workspaces/ML_Service_Demo/PipelineRuns/PipelineSubmit/ddbf8582-5f16-4889-ba27-36379028eb20)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "published_pipeline = pipeline_run.publish_pipeline(\n",
    "    name=\"Inception_v3_scoring\", description=\"Batch scoring using Inception v3 model\", version=\"1.0\")\n",
    "\n",
    "published_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run the pipeline from the REST endpoint, you first need an OAuth2 Bearer-type authentication header. This example uses interactive authentication for illustration purposes, but for most production scenarios requiring automated or headless authentication, use service principle authentication as [described in this notebook](https://aka.ms/pl-restep-auth).\n",
    "\n",
    "Service principle authentication involves creating an **App Registration** in **Azure Active Directory**, generating a client secret, and then granting your service principal **role access** to your machine learning workspace. You then use the [`ServicePrincipalAuthentication`](https://docs.microsoft.com/python/api/azureml-core/azureml.core.authentication.serviceprincipalauthentication?view=azure-ml-py) class to manage your auth flow. \n",
    "\n",
    "Both `InteractiveLoginAuthentication` and `ServicePrincipalAuthentication` inherit from `AbstractAuthentication`, and in both cases you use the `get_authentication_header()` function in the same way to fetch the header."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.authentication import InteractiveLoginAuthentication\n",
    "\n",
    "interactive_auth = InteractiveLoginAuthentication()\n",
    "auth_header = interactive_auth.get_authentication_header()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the REST url from the `endpoint` property of the published pipeline object. You can also find the REST url in your workspace in the portal. Build an HTTP POST request to the endpoint, specifying your authentication header. Additionally, add a JSON payload object with the experiment name and the batch size parameter. As a reminder, the `param_batch_size` is passed through to your `batch_scoring.py` script because you defined it as a `PipelineParameter` object in the step configuration.\n",
    "\n",
    "Make the request to trigger the run. Access the `Id` key from the response dict to get the value of the run id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "rest_endpoint = published_pipeline.endpoint\n",
    "response = requests.post(rest_endpoint, \n",
    "                         headers=auth_header, \n",
    "                         json={\"ExperimentName\": \"batch_scoring\",\n",
    "                               \"ParameterAssignments\": {\"param_batch_size\": 50}})\n",
    "run_id = response.json()[\"Id\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the run id to monitor the status of the new run. This will take another 10-15 min to run and will look similar to the previous pipeline run, so if you don't need to see another pipeline run, you can skip watching the full output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd273aa47f26462fb26f52e2a7a3c3b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "_PipelineWidget(widget_settings={'childWidgetDisplay': 'popup', 'send_telemetry': False, 'log_level': 'INFO', …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from azureml.pipeline.core.run import PipelineRun\n",
    "from azureml.widgets import RunDetails\n",
    "\n",
    "published_pipeline_run = PipelineRun(ws.experiments[\"batch_scoring\"], run_id)\n",
    "RunDetails(published_pipeline_run).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up resources\n",
    "\n",
    "Do not complete this section if you plan on running other Azure Machine Learning service tutorials.\n",
    "\n",
    "### Stop the notebook VM\n",
    "\n",
    "If you used a cloud notebook server, stop the VM when you are not using it to reduce cost.\n",
    "\n",
    "1. In your workspace, select **Notebook VMs**.\n",
    "1. From the list, select the VM.\n",
    "1. Select **Stop**.\n",
    "1. When you're ready to use the server again, select **Start**.\n",
    "\n",
    "### Delete everything\n",
    "\n",
    "If you don't plan to use the resources you created, delete them, so you don't incur any charges.\n",
    "\n",
    "1. In the Azure portal, select **Resource groups** on the far left.\n",
    "1. From the list, select the resource group you created.\n",
    "1. Select **Delete resource group**.\n",
    "1. Enter the resource group name. Then select **Delete**.\n",
    "\n",
    "You can also keep the resource group but delete a single workspace. Display the workspace properties and select **Delete**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next steps\n",
    "\n",
    "In this machine learning pipelines tutorial, you did the following tasks:\n",
    "\n",
    "> * Built a pipeline with environment dependencies to run on a remote GPU compute resource\n",
    "> * Created a scoring script to run batch predictions with a pre-trained Tensorflow model\n",
    "> * Published a pipeline and enabled it to be run from a REST endpoint\n",
    "\n",
    "See the [how-to](https://docs.microsoft.com/azure/machine-learning/service/how-to-create-your-first-pipeline?view=azure-devops) for additional detail on building pipelines with the machine learning SDK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "authors": [
   {
    "name": "sanpil"
   }
  ],
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "msauthor": "trbye"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
